{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[50000:]\n",
    "Y_val = Y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(50000, 784).astype('float32') / 255.0\n",
    "X_val = X_val.reshape(10000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rand_idxs = np.random.choice(50000, 700)\n",
    "val_rand_idxs = np.random.choice(10000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5994,  1688, 11513, 48056, 25544, 39168, 25365, 11261, 19091,\n",
       "       30410, 43883, 23545, 39829,  1447,  6542, 19960,  3610, 34266,\n",
       "        2710, 25410,  8127, 44348, 12929,  4019,   858, 24133,  7016,\n",
       "        4718, 30305,  6557, 42602, 31896, 47611, 37694, 10759, 31736,\n",
       "       11435,  7713, 22907, 46415, 20656,  7188, 15198,  5525, 33151,\n",
       "        8055, 22070, 17408,  8896,  8466, 17780, 14271, 16037, 35000,\n",
       "       30648, 42749, 16046, 33313,  2161, 11265,  8021, 45173, 45943,\n",
       "       30666, 16107, 40828, 19428,  2779, 23039, 43920, 37874, 43282,\n",
       "       40779, 34999, 41824,  7391, 20621, 17822, 36400, 14269, 31028,\n",
       "       43505, 29180, 17138, 34818, 38044, 22564, 49836, 47280, 30523,\n",
       "        8374,  1520, 16868, 28251, 12565, 21105, 18232, 40552,  8487,\n",
       "        6429,  1712, 25609, 23585,  9711,  2492, 27959, 37131,  3284,\n",
       "       43914, 37232, 42704, 43084,  4932, 19756, 26668,  1019, 48016,\n",
       "       28327, 42162,  6081, 26659, 23725, 24818, 28724, 36097, 26386,\n",
       "       16319, 39938, 24279, 41443,  9364, 24529,  7190, 17387,  8839,\n",
       "       13793,  5035,  6688,  9820, 23356, 46992,  4891, 31029,   363,\n",
       "       23048,  4884, 45832,  5784, 13262,  7756, 18532, 40841, 25266,\n",
       "       18492, 49285, 14548, 12791, 16873, 42602, 15697, 31753, 40077,\n",
       "       34302, 28716, 38471, 17244, 13604, 24490, 35016, 15508, 25662,\n",
       "       22700, 36510,  8849, 49524, 12320, 22197, 10791,    32, 48599,\n",
       "       40316, 35157, 27955, 25690, 14543, 14684, 26379, 18166, 32233,\n",
       "       45073,  8627,  6157, 21087, 46020,  1023,  6207, 23849,  4045,\n",
       "       18016, 49880, 13372, 40879, 32164, 19127,  2282, 30392, 24277,\n",
       "       19799, 25052, 31644, 20847, 36462,  2955, 28707, 37077,  6364,\n",
       "       15153, 23624, 41177, 27682, 13579, 42488, 40409, 15035, 37764,\n",
       "       20053,  8120, 23646, 18093, 41825, 30869,  9378, 22266,  7709,\n",
       "       49578, 29447, 28382, 42316, 33959, 28473, 18677, 18284, 24236,\n",
       "       24853, 41513, 41692,  1058, 33423, 34605, 12580, 44968, 13914,\n",
       "       14900,  6120, 10565, 37302, 14220, 10266,  6580, 30434, 37163,\n",
       "       19319, 36503, 29106,  6464,  1973, 10874, 36819,  4378, 20739,\n",
       "         277, 25198, 36340, 19066, 16279,  5613, 31564, 30482, 31522,\n",
       "       21340, 33427, 46396, 48502, 48871,   767,  4717, 46631,  4672,\n",
       "       34364, 42892,  1960, 44921, 13114,  5854, 19861, 19180, 47360,\n",
       "       27171, 28749,  9237,  8772, 13282, 35909, 42353,  4071, 40361,\n",
       "       23494, 31963, 17056, 15937, 20703, 38343, 20960, 46039, 15613,\n",
       "        6425,  6771, 45594, 26062, 18443, 37662, 34127, 19701, 12163,\n",
       "       47406, 31279, 16270, 13829, 45526, 42022, 31794, 19643,  1298,\n",
       "       17181, 34583, 14609, 33694,  7460,  7687, 48202,  3683,  4465,\n",
       "       39603,  6657, 13557, 13396,  7989, 12782, 25722, 46674, 38028,\n",
       "       24772, 28419, 48830, 24881, 39722, 47364, 47253, 38728, 17921,\n",
       "        3730, 27110, 34999,  2481,   219, 29003, 39679, 30211,  3161,\n",
       "        6419,  5845,  8688, 49319,  2146, 24492,  1342, 18672, 44411,\n",
       "       26335, 44385, 47744, 19492, 43318, 46245,   998, 10218, 29348,\n",
       "        7513,  4461, 21472, 44372,  1426,  7370, 27993,  5420,  1823,\n",
       "       23771, 46540, 32616, 19083,  1906, 44015, 39677, 24666, 36762,\n",
       "       36612,  1408, 33044, 28471, 23690,  1339, 13166,  9910, 23229,\n",
       "       16468, 13478, 44411,  2515,  4192, 22099, 14276, 40139, 43438,\n",
       "       12529,  3889, 12168,  2329, 25059, 42592, 23820, 45012,  8296,\n",
       "        2412,  3988, 13027,  3765,  1565, 24480, 31064, 31367, 14740,\n",
       "       12771,  7935, 45507, 45974, 14577, 31255, 32009, 14113, 22543,\n",
       "       49293, 27502, 26706, 48540, 20397,  6124,  7664, 12359, 41073,\n",
       "       31810, 47390, 33473, 41235, 46522, 39083, 34580,  2204, 10458,\n",
       "       13736, 42192,  9438,  5398, 43096, 41379, 30713, 26335, 33367,\n",
       "       19710, 18232, 21052, 22546, 35391, 12844, 39210, 44056, 10975,\n",
       "       35628,  7840,  6824, 24136, 13366, 46132, 20846, 43699, 32736,\n",
       "       21836,    44, 17380, 26877, 12720, 14837, 15258,  6002,  5890,\n",
       "       46918, 19614, 14144, 33045, 39365, 18230, 11417, 38789, 11245,\n",
       "       28701, 48302, 48564,  4143, 31769, 23934, 11374, 48918, 48978,\n",
       "        2529, 28212, 42840, 22040, 32174, 21049, 22548, 18333, 22406,\n",
       "       21139, 13562, 36790, 19505,  4045,  9558,  9128,  4576,   161,\n",
       "       46685,  2580,  6868, 28152, 47623, 39592, 40587, 26014,  4493,\n",
       "       35468,  8576, 15203, 35848,  4591,  7635, 15256, 31713, 30069,\n",
       "       16059, 13352, 33075, 30526, 44181, 28863,  1929, 24971,   129,\n",
       "        5404, 39852, 12180, 24882, 32633, 43321, 26539, 28872, 46355,\n",
       "       25889, 16925, 42098, 14365, 22302,  9858, 18731, 49797, 19739,\n",
       "       20714,  2691, 22195, 31652, 37926, 34718,    66, 40422, 13060,\n",
       "       36052,  4641,  7146, 40775, 26252, 13857, 21163, 32030, 13120,\n",
       "       45401,  8854, 25088, 12617, 22843,  1350, 19505, 23100, 19392,\n",
       "       36669, 13152, 37228, 19128, 10580, 37676, 43860, 47147, 12473,\n",
       "        8092, 15885, 23228, 12545, 34837, 25841, 13542,   827, 20918,\n",
       "       37015, 15591,  2394, 30840, 27714,  9397, 31523, 24184, 34872,\n",
       "       34701, 18368, 38226, 11479, 34359, 34235,   390, 46927, 12954,\n",
       "         251, 20171, 45891, 23908,  5471,  5442, 10346,  8865,  4685,\n",
       "       48165, 15470, 39739, 16689, 10639, 33848, 30097,  7535, 33293,\n",
       "       12214, 47684,  2976, 39540, 12822, 21721,  9677, 34793, 23313,\n",
       "       14385, 22814, 24124, 18655, 30438, 27967, 47907, 44397, 27365,\n",
       "         233,  8483, 25458,  9110, 45809, 16227, 20946, 21353,  5977,\n",
       "       49036,  3603, 45699, 34460, 41847, 37326, 36243])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rand_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[train_rand_idxs]\n",
    "Y_train = Y_train[train_rand_idxs]\n",
    "X_val = X_val[val_rand_idxs]\n",
    "Y_val = Y_val[val_rand_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "Y_test = np_utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.2617 - accuracy: 0.1429 - val_loss: 2.2369 - val_accuracy: 0.1733\n",
      "Epoch 2/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 2.1772 - accuracy: 0.2071 - val_loss: 2.1678 - val_accuracy: 0.1900\n",
      "Epoch 3/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 2.1043 - accuracy: 0.2271 - val_loss: 2.1145 - val_accuracy: 0.2100\n",
      "Epoch 4/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 2.0437 - accuracy: 0.2371 - val_loss: 2.0612 - val_accuracy: 0.2067\n",
      "Epoch 5/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.9883 - accuracy: 0.2457 - val_loss: 2.0138 - val_accuracy: 0.2100\n",
      "Epoch 6/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.9375 - accuracy: 0.2657 - val_loss: 1.9703 - val_accuracy: 0.2233\n",
      "Epoch 7/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.8904 - accuracy: 0.2814 - val_loss: 1.9312 - val_accuracy: 0.2333\n",
      "Epoch 8/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.8489 - accuracy: 0.3029 - val_loss: 1.8924 - val_accuracy: 0.2267\n",
      "Epoch 9/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.8093 - accuracy: 0.3114 - val_loss: 1.8584 - val_accuracy: 0.2367\n",
      "Epoch 10/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.7742 - accuracy: 0.3186 - val_loss: 1.8271 - val_accuracy: 0.2700\n",
      "Epoch 11/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.7428 - accuracy: 0.3371 - val_loss: 1.8001 - val_accuracy: 0.2700\n",
      "Epoch 12/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.7144 - accuracy: 0.3371 - val_loss: 1.7738 - val_accuracy: 0.2933\n",
      "Epoch 13/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.6858 - accuracy: 0.3514 - val_loss: 1.7515 - val_accuracy: 0.2900\n",
      "Epoch 14/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.6608 - accuracy: 0.3500 - val_loss: 1.7300 - val_accuracy: 0.2967\n",
      "Epoch 15/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.6368 - accuracy: 0.3714 - val_loss: 1.7118 - val_accuracy: 0.3067\n",
      "Epoch 16/300\n",
      "70/70 [==============================] - 0s 972us/step - loss: 1.6152 - accuracy: 0.3743 - val_loss: 1.6995 - val_accuracy: 0.3333\n",
      "Epoch 17/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5955 - accuracy: 0.3886 - val_loss: 1.6823 - val_accuracy: 0.3300\n",
      "Epoch 18/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.5750 - accuracy: 0.3943 - val_loss: 1.6673 - val_accuracy: 0.3467\n",
      "Epoch 19/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5562 - accuracy: 0.4057 - val_loss: 1.6479 - val_accuracy: 0.3500\n",
      "Epoch 20/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.5396 - accuracy: 0.4057 - val_loss: 1.6365 - val_accuracy: 0.3500\n",
      "Epoch 21/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5222 - accuracy: 0.4329 - val_loss: 1.6217 - val_accuracy: 0.3533\n",
      "Epoch 22/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5051 - accuracy: 0.4243 - val_loss: 1.6105 - val_accuracy: 0.3667\n",
      "Epoch 23/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.4896 - accuracy: 0.4314 - val_loss: 1.6002 - val_accuracy: 0.3667\n",
      "Epoch 24/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.4758 - accuracy: 0.4343 - val_loss: 1.5849 - val_accuracy: 0.3733\n",
      "Epoch 25/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.4617 - accuracy: 0.4286 - val_loss: 1.5760 - val_accuracy: 0.3767\n",
      "Epoch 26/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.4487 - accuracy: 0.4386 - val_loss: 1.5661 - val_accuracy: 0.3767\n",
      "Epoch 27/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.4353 - accuracy: 0.4457 - val_loss: 1.5563 - val_accuracy: 0.3833\n",
      "Epoch 28/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.4231 - accuracy: 0.4543 - val_loss: 1.5467 - val_accuracy: 0.3833\n",
      "Epoch 29/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.4106 - accuracy: 0.4629 - val_loss: 1.5391 - val_accuracy: 0.3800\n",
      "Epoch 30/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.4003 - accuracy: 0.4586 - val_loss: 1.5319 - val_accuracy: 0.3833\n",
      "Epoch 31/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.3875 - accuracy: 0.4586 - val_loss: 1.5272 - val_accuracy: 0.3700\n",
      "Epoch 32/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3777 - accuracy: 0.4557 - val_loss: 1.5220 - val_accuracy: 0.3800\n",
      "Epoch 33/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.3664 - accuracy: 0.4714 - val_loss: 1.5108 - val_accuracy: 0.3900\n",
      "Epoch 34/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3568 - accuracy: 0.4829 - val_loss: 1.5090 - val_accuracy: 0.3833\n",
      "Epoch 35/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3470 - accuracy: 0.4971 - val_loss: 1.5019 - val_accuracy: 0.3900\n",
      "Epoch 36/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.3383 - accuracy: 0.4857 - val_loss: 1.4899 - val_accuracy: 0.4067\n",
      "Epoch 37/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3298 - accuracy: 0.4929 - val_loss: 1.4889 - val_accuracy: 0.4100\n",
      "Epoch 38/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 1.3217 - accuracy: 0.5029 - val_loss: 1.4807 - val_accuracy: 0.4000\n",
      "Epoch 39/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.3113 - accuracy: 0.5057 - val_loss: 1.4764 - val_accuracy: 0.4033\n",
      "Epoch 40/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.3037 - accuracy: 0.5143 - val_loss: 1.4732 - val_accuracy: 0.4100\n",
      "Epoch 41/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.2947 - accuracy: 0.5143 - val_loss: 1.4689 - val_accuracy: 0.3967\n",
      "Epoch 42/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.2870 - accuracy: 0.5043 - val_loss: 1.4629 - val_accuracy: 0.4300\n",
      "Epoch 43/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.2807 - accuracy: 0.5129 - val_loss: 1.4583 - val_accuracy: 0.4167\n",
      "Epoch 44/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 1.2731 - accuracy: 0.5257 - val_loss: 1.4539 - val_accuracy: 0.4233\n",
      "Epoch 45/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.2650 - accuracy: 0.5029 - val_loss: 1.4498 - val_accuracy: 0.4267\n",
      "Epoch 46/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 1.2597 - accuracy: 0.5300 - val_loss: 1.4454 - val_accuracy: 0.4267\n",
      "Epoch 47/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.2517 - accuracy: 0.5214 - val_loss: 1.4480 - val_accuracy: 0.4067\n",
      "Epoch 48/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.2466 - accuracy: 0.5257 - val_loss: 1.4375 - val_accuracy: 0.4267\n",
      "Epoch 49/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.2381 - accuracy: 0.5314 - val_loss: 1.4385 - val_accuracy: 0.4267\n",
      "Epoch 50/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 1.2333 - accuracy: 0.5329 - val_loss: 1.4310 - val_accuracy: 0.4233\n",
      "Epoch 51/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.2271 - accuracy: 0.5314 - val_loss: 1.4304 - val_accuracy: 0.4233\n",
      "Epoch 52/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.2194 - accuracy: 0.5257 - val_loss: 1.4218 - val_accuracy: 0.4400\n",
      "Epoch 53/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.2141 - accuracy: 0.5443 - val_loss: 1.4296 - val_accuracy: 0.4233\n",
      "Epoch 54/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.2083 - accuracy: 0.5357 - val_loss: 1.4162 - val_accuracy: 0.4500\n",
      "Epoch 55/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 1.2019 - accuracy: 0.5400 - val_loss: 1.4160 - val_accuracy: 0.4467\n",
      "Epoch 56/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.1984 - accuracy: 0.5429 - val_loss: 1.4158 - val_accuracy: 0.4400\n",
      "Epoch 57/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.1918 - accuracy: 0.5471 - val_loss: 1.4106 - val_accuracy: 0.4233\n",
      "Epoch 58/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.1873 - accuracy: 0.5414 - val_loss: 1.4103 - val_accuracy: 0.4500\n",
      "Epoch 59/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1815 - accuracy: 0.5500 - val_loss: 1.4064 - val_accuracy: 0.4367\n",
      "Epoch 60/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.1765 - accuracy: 0.5500 - val_loss: 1.4011 - val_accuracy: 0.4733\n",
      "Epoch 61/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.1711 - accuracy: 0.5486 - val_loss: 1.4006 - val_accuracy: 0.4500\n",
      "Epoch 62/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.1672 - accuracy: 0.5629 - val_loss: 1.3994 - val_accuracy: 0.4733\n",
      "Epoch 63/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.1643 - accuracy: 0.5543 - val_loss: 1.3985 - val_accuracy: 0.4600\n",
      "Epoch 64/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.1572 - accuracy: 0.5600 - val_loss: 1.3954 - val_accuracy: 0.4567\n",
      "Epoch 65/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.1537 - accuracy: 0.5586 - val_loss: 1.3934 - val_accuracy: 0.4667\n",
      "Epoch 66/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.1491 - accuracy: 0.5529 - val_loss: 1.3961 - val_accuracy: 0.4367\n",
      "Epoch 67/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1439 - accuracy: 0.5571 - val_loss: 1.3941 - val_accuracy: 0.4433\n",
      "Epoch 68/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.1393 - accuracy: 0.5500 - val_loss: 1.3872 - val_accuracy: 0.4733\n",
      "Epoch 69/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.1348 - accuracy: 0.5614 - val_loss: 1.3880 - val_accuracy: 0.4667\n",
      "Epoch 70/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.1322 - accuracy: 0.5529 - val_loss: 1.3869 - val_accuracy: 0.4800\n",
      "Epoch 71/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 1.1275 - accuracy: 0.5629 - val_loss: 1.3878 - val_accuracy: 0.4433\n",
      "Epoch 72/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.1238 - accuracy: 0.5657 - val_loss: 1.3863 - val_accuracy: 0.4533\n",
      "Epoch 73/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.1205 - accuracy: 0.5629 - val_loss: 1.3827 - val_accuracy: 0.4633\n",
      "Epoch 74/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.1143 - accuracy: 0.5643 - val_loss: 1.3845 - val_accuracy: 0.4600\n",
      "Epoch 75/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.1111 - accuracy: 0.5643 - val_loss: 1.3818 - val_accuracy: 0.4600\n",
      "Epoch 76/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.1081 - accuracy: 0.5586 - val_loss: 1.3833 - val_accuracy: 0.4433\n",
      "Epoch 77/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.1043 - accuracy: 0.5629 - val_loss: 1.3799 - val_accuracy: 0.4667\n",
      "Epoch 78/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1006 - accuracy: 0.5786 - val_loss: 1.3825 - val_accuracy: 0.4500\n",
      "Epoch 79/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0963 - accuracy: 0.5786 - val_loss: 1.3817 - val_accuracy: 0.4400\n",
      "Epoch 80/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0923 - accuracy: 0.5700 - val_loss: 1.3754 - val_accuracy: 0.4500\n",
      "Epoch 81/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0893 - accuracy: 0.5714 - val_loss: 1.3750 - val_accuracy: 0.4700\n",
      "Epoch 82/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0847 - accuracy: 0.5671 - val_loss: 1.3803 - val_accuracy: 0.4533\n",
      "Epoch 83/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0843 - accuracy: 0.5714 - val_loss: 1.3758 - val_accuracy: 0.4633\n",
      "Epoch 84/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0792 - accuracy: 0.5671 - val_loss: 1.3749 - val_accuracy: 0.4733\n",
      "Epoch 85/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.0768 - accuracy: 0.5843 - val_loss: 1.3712 - val_accuracy: 0.4700\n",
      "Epoch 86/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.0732 - accuracy: 0.5800 - val_loss: 1.3743 - val_accuracy: 0.4533\n",
      "Epoch 87/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0699 - accuracy: 0.5857 - val_loss: 1.3777 - val_accuracy: 0.4600\n",
      "Epoch 88/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.0658 - accuracy: 0.5729 - val_loss: 1.3679 - val_accuracy: 0.4833\n",
      "Epoch 89/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0642 - accuracy: 0.5857 - val_loss: 1.3724 - val_accuracy: 0.4800\n",
      "Epoch 90/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0605 - accuracy: 0.5829 - val_loss: 1.3722 - val_accuracy: 0.4567\n",
      "Epoch 91/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.0577 - accuracy: 0.5757 - val_loss: 1.3696 - val_accuracy: 0.4700\n",
      "Epoch 92/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0541 - accuracy: 0.5857 - val_loss: 1.3807 - val_accuracy: 0.4367\n",
      "Epoch 93/300\n",
      "70/70 [==============================] - 0s 972us/step - loss: 1.0527 - accuracy: 0.5771 - val_loss: 1.3698 - val_accuracy: 0.4800\n",
      "Epoch 94/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.0483 - accuracy: 0.5843 - val_loss: 1.3793 - val_accuracy: 0.4467\n",
      "Epoch 95/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.0458 - accuracy: 0.5857 - val_loss: 1.3740 - val_accuracy: 0.4500\n",
      "Epoch 96/300\n",
      "70/70 [==============================] - 0s 972us/step - loss: 1.0438 - accuracy: 0.5786 - val_loss: 1.3686 - val_accuracy: 0.4767\n",
      "Epoch 97/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0398 - accuracy: 0.5886 - val_loss: 1.3668 - val_accuracy: 0.4900\n",
      "Epoch 98/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0380 - accuracy: 0.5814 - val_loss: 1.3670 - val_accuracy: 0.4700\n",
      "Epoch 99/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.0349 - accuracy: 0.5857 - val_loss: 1.3695 - val_accuracy: 0.4633\n",
      "Epoch 100/300\n",
      "70/70 [==============================] - 0s 843us/step - loss: 1.0328 - accuracy: 0.5886 - val_loss: 1.3724 - val_accuracy: 0.4633\n",
      "Epoch 101/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.0303 - accuracy: 0.5957 - val_loss: 1.3680 - val_accuracy: 0.4767\n",
      "Epoch 102/300\n",
      "70/70 [==============================] - 0s 972us/step - loss: 1.0264 - accuracy: 0.5929 - val_loss: 1.3672 - val_accuracy: 0.4800\n",
      "Epoch 103/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0236 - accuracy: 0.6057 - val_loss: 1.3710 - val_accuracy: 0.4667\n",
      "Epoch 104/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.0223 - accuracy: 0.5943 - val_loss: 1.3676 - val_accuracy: 0.4733\n",
      "Epoch 105/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.0201 - accuracy: 0.5971 - val_loss: 1.3679 - val_accuracy: 0.4733\n",
      "Epoch 106/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0181 - accuracy: 0.5929 - val_loss: 1.3729 - val_accuracy: 0.4633\n",
      "Epoch 107/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.0135 - accuracy: 0.6086 - val_loss: 1.3685 - val_accuracy: 0.4767\n",
      "Epoch 108/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 1.0098 - accuracy: 0.6014 - val_loss: 1.3707 - val_accuracy: 0.4600\n",
      "Epoch 109/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.0089 - accuracy: 0.5914 - val_loss: 1.3698 - val_accuracy: 0.4767\n",
      "Epoch 110/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.0072 - accuracy: 0.6000 - val_loss: 1.3726 - val_accuracy: 0.4833\n",
      "Epoch 111/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.0033 - accuracy: 0.6157 - val_loss: 1.3834 - val_accuracy: 0.4700\n",
      "Epoch 112/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.0023 - accuracy: 0.5971 - val_loss: 1.3648 - val_accuracy: 0.4767\n",
      "Epoch 113/300\n",
      "70/70 [==============================] - 0s 972us/step - loss: 1.0003 - accuracy: 0.6057 - val_loss: 1.3687 - val_accuracy: 0.4733\n",
      "Epoch 114/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.9969 - accuracy: 0.6029 - val_loss: 1.3678 - val_accuracy: 0.4600\n",
      "Epoch 115/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 857us/step - loss: 0.9962 - accuracy: 0.6086 - val_loss: 1.3637 - val_accuracy: 0.4800\n",
      "Epoch 116/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9928 - accuracy: 0.5986 - val_loss: 1.3700 - val_accuracy: 0.4733\n",
      "Epoch 117/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.9897 - accuracy: 0.6157 - val_loss: 1.3826 - val_accuracy: 0.4500\n",
      "Epoch 118/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.9888 - accuracy: 0.6143 - val_loss: 1.3772 - val_accuracy: 0.4667\n",
      "Epoch 119/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9866 - accuracy: 0.6043 - val_loss: 1.3778 - val_accuracy: 0.4667\n",
      "Epoch 120/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.9865 - accuracy: 0.6129 - val_loss: 1.3770 - val_accuracy: 0.4567\n",
      "Epoch 121/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9828 - accuracy: 0.5914 - val_loss: 1.3709 - val_accuracy: 0.4833\n",
      "Epoch 122/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.9814 - accuracy: 0.6071 - val_loss: 1.3799 - val_accuracy: 0.4700\n",
      "Epoch 123/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9772 - accuracy: 0.6100 - val_loss: 1.3774 - val_accuracy: 0.5100\n",
      "Epoch 124/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9760 - accuracy: 0.6100 - val_loss: 1.3801 - val_accuracy: 0.4933\n",
      "Epoch 125/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9739 - accuracy: 0.6243 - val_loss: 1.3709 - val_accuracy: 0.5000\n",
      "Epoch 126/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9711 - accuracy: 0.6086 - val_loss: 1.3848 - val_accuracy: 0.4967\n",
      "Epoch 127/300\n",
      "70/70 [==============================] - 0s 843us/step - loss: 0.9690 - accuracy: 0.6200 - val_loss: 1.3864 - val_accuracy: 0.5000\n",
      "Epoch 128/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.9677 - accuracy: 0.6200 - val_loss: 1.3949 - val_accuracy: 0.4900\n",
      "Epoch 129/300\n",
      "70/70 [==============================] - 0s 1000us/step - loss: 0.9662 - accuracy: 0.6157 - val_loss: 1.3858 - val_accuracy: 0.4867\n",
      "Epoch 130/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9638 - accuracy: 0.6243 - val_loss: 1.3788 - val_accuracy: 0.5000\n",
      "Epoch 131/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.9625 - accuracy: 0.6286 - val_loss: 1.3768 - val_accuracy: 0.4900\n",
      "Epoch 132/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9586 - accuracy: 0.6229 - val_loss: 1.3791 - val_accuracy: 0.4833\n",
      "Epoch 133/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.9585 - accuracy: 0.6357 - val_loss: 1.3840 - val_accuracy: 0.4967\n",
      "Epoch 134/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.9560 - accuracy: 0.6300 - val_loss: 1.3806 - val_accuracy: 0.5000\n",
      "Epoch 135/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.9526 - accuracy: 0.6243 - val_loss: 1.3782 - val_accuracy: 0.5133\n",
      "Epoch 136/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9510 - accuracy: 0.6300 - val_loss: 1.3872 - val_accuracy: 0.5000\n",
      "Epoch 137/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9487 - accuracy: 0.6200 - val_loss: 1.3799 - val_accuracy: 0.5133\n",
      "Epoch 138/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9478 - accuracy: 0.6286 - val_loss: 1.3844 - val_accuracy: 0.5033\n",
      "Epoch 139/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.9459 - accuracy: 0.6329 - val_loss: 1.3801 - val_accuracy: 0.5067\n",
      "Epoch 140/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9430 - accuracy: 0.6257 - val_loss: 1.3873 - val_accuracy: 0.5033\n",
      "Epoch 141/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9426 - accuracy: 0.6314 - val_loss: 1.3818 - val_accuracy: 0.5033\n",
      "Epoch 142/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.9393 - accuracy: 0.6486 - val_loss: 1.3949 - val_accuracy: 0.5100\n",
      "Epoch 143/300\n",
      "70/70 [==============================] - 0s 843us/step - loss: 0.9388 - accuracy: 0.6300 - val_loss: 1.3939 - val_accuracy: 0.5000\n",
      "Epoch 144/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.9359 - accuracy: 0.6529 - val_loss: 1.3980 - val_accuracy: 0.5067\n",
      "Epoch 145/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9343 - accuracy: 0.6400 - val_loss: 1.3858 - val_accuracy: 0.5100\n",
      "Epoch 146/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9332 - accuracy: 0.6343 - val_loss: 1.3854 - val_accuracy: 0.4900\n",
      "Epoch 147/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.9324 - accuracy: 0.6443 - val_loss: 1.3877 - val_accuracy: 0.5100\n",
      "Epoch 148/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.9286 - accuracy: 0.6371 - val_loss: 1.4010 - val_accuracy: 0.5100\n",
      "Epoch 149/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9281 - accuracy: 0.6343 - val_loss: 1.3957 - val_accuracy: 0.5100\n",
      "Epoch 150/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.9267 - accuracy: 0.6414 - val_loss: 1.4083 - val_accuracy: 0.4967\n",
      "Epoch 151/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9229 - accuracy: 0.6443 - val_loss: 1.3951 - val_accuracy: 0.5067\n",
      "Epoch 152/300\n",
      "70/70 [==============================] - 0s 829us/step - loss: 0.9217 - accuracy: 0.6371 - val_loss: 1.3992 - val_accuracy: 0.5133\n",
      "Epoch 153/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9201 - accuracy: 0.6414 - val_loss: 1.3902 - val_accuracy: 0.5200\n",
      "Epoch 154/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.9183 - accuracy: 0.6529 - val_loss: 1.4002 - val_accuracy: 0.5067\n",
      "Epoch 155/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.9167 - accuracy: 0.6486 - val_loss: 1.4036 - val_accuracy: 0.5200\n",
      "Epoch 156/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.9158 - accuracy: 0.6343 - val_loss: 1.4060 - val_accuracy: 0.5133\n",
      "Epoch 157/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.9147 - accuracy: 0.6443 - val_loss: 1.4014 - val_accuracy: 0.5200\n",
      "Epoch 158/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.9129 - accuracy: 0.6429 - val_loss: 1.4012 - val_accuracy: 0.5267\n",
      "Epoch 159/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9104 - accuracy: 0.6400 - val_loss: 1.4063 - val_accuracy: 0.5133\n",
      "Epoch 160/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9093 - accuracy: 0.6600 - val_loss: 1.4058 - val_accuracy: 0.5233\n",
      "Epoch 161/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.9069 - accuracy: 0.6443 - val_loss: 1.4044 - val_accuracy: 0.5167\n",
      "Epoch 162/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9057 - accuracy: 0.6471 - val_loss: 1.4053 - val_accuracy: 0.5100\n",
      "Epoch 163/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9057 - accuracy: 0.6543 - val_loss: 1.4005 - val_accuracy: 0.5300\n",
      "Epoch 164/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.9021 - accuracy: 0.6600 - val_loss: 1.4133 - val_accuracy: 0.5233\n",
      "Epoch 165/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.9029 - accuracy: 0.6514 - val_loss: 1.4008 - val_accuracy: 0.5333\n",
      "Epoch 166/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8999 - accuracy: 0.6500 - val_loss: 1.4121 - val_accuracy: 0.5367\n",
      "Epoch 167/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8992 - accuracy: 0.6443 - val_loss: 1.4021 - val_accuracy: 0.5267\n",
      "Epoch 168/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8969 - accuracy: 0.6543 - val_loss: 1.4137 - val_accuracy: 0.5233\n",
      "Epoch 169/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8954 - accuracy: 0.6514 - val_loss: 1.4120 - val_accuracy: 0.5167\n",
      "Epoch 170/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8952 - accuracy: 0.6500 - val_loss: 1.4246 - val_accuracy: 0.5067\n",
      "Epoch 171/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.8928 - accuracy: 0.6586 - val_loss: 1.4090 - val_accuracy: 0.5367\n",
      "Epoch 172/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8923 - accuracy: 0.6486 - val_loss: 1.4212 - val_accuracy: 0.5233\n",
      "Epoch 173/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8896 - accuracy: 0.6557 - val_loss: 1.4217 - val_accuracy: 0.5233\n",
      "Epoch 174/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8889 - accuracy: 0.6514 - val_loss: 1.4220 - val_accuracy: 0.5267\n",
      "Epoch 175/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8860 - accuracy: 0.6657 - val_loss: 1.4127 - val_accuracy: 0.4967\n",
      "Epoch 176/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8870 - accuracy: 0.6629 - val_loss: 1.4230 - val_accuracy: 0.5167\n",
      "Epoch 177/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8834 - accuracy: 0.6586 - val_loss: 1.4305 - val_accuracy: 0.5100\n",
      "Epoch 178/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8821 - accuracy: 0.6657 - val_loss: 1.4252 - val_accuracy: 0.5267\n",
      "Epoch 179/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8811 - accuracy: 0.6629 - val_loss: 1.4251 - val_accuracy: 0.5267\n",
      "Epoch 180/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.8801 - accuracy: 0.6686 - val_loss: 1.4298 - val_accuracy: 0.5300\n",
      "Epoch 181/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8797 - accuracy: 0.6557 - val_loss: 1.4257 - val_accuracy: 0.5133\n",
      "Epoch 182/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8779 - accuracy: 0.6486 - val_loss: 1.4289 - val_accuracy: 0.5167\n",
      "Epoch 183/300\n",
      "70/70 [==============================] - 0s 843us/step - loss: 0.8762 - accuracy: 0.6586 - val_loss: 1.4232 - val_accuracy: 0.5200\n",
      "Epoch 184/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8745 - accuracy: 0.6586 - val_loss: 1.4414 - val_accuracy: 0.5333\n",
      "Epoch 185/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8739 - accuracy: 0.6629 - val_loss: 1.4278 - val_accuracy: 0.5133\n",
      "Epoch 186/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.8723 - accuracy: 0.6643 - val_loss: 1.4294 - val_accuracy: 0.5267\n",
      "Epoch 187/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8682 - accuracy: 0.6814 - val_loss: 1.4306 - val_accuracy: 0.4900\n",
      "Epoch 188/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.8707 - accuracy: 0.6700 - val_loss: 1.4331 - val_accuracy: 0.5267\n",
      "Epoch 189/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8675 - accuracy: 0.6657 - val_loss: 1.4402 - val_accuracy: 0.5133\n",
      "Epoch 190/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.8680 - accuracy: 0.6629 - val_loss: 1.4347 - val_accuracy: 0.5100\n",
      "Epoch 191/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8663 - accuracy: 0.6657 - val_loss: 1.4465 - val_accuracy: 0.5267\n",
      "Epoch 192/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8640 - accuracy: 0.6671 - val_loss: 1.4511 - val_accuracy: 0.5267\n",
      "Epoch 193/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.8631 - accuracy: 0.6600 - val_loss: 1.4328 - val_accuracy: 0.5200\n",
      "Epoch 194/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.8607 - accuracy: 0.6729 - val_loss: 1.4497 - val_accuracy: 0.5067\n",
      "Epoch 195/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.8604 - accuracy: 0.6729 - val_loss: 1.4466 - val_accuracy: 0.5200\n",
      "Epoch 196/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.8584 - accuracy: 0.6657 - val_loss: 1.4493 - val_accuracy: 0.5033\n",
      "Epoch 197/300\n",
      "70/70 [==============================] - 0s 829us/step - loss: 0.8569 - accuracy: 0.6629 - val_loss: 1.4421 - val_accuracy: 0.5133\n",
      "Epoch 198/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.8544 - accuracy: 0.6714 - val_loss: 1.4607 - val_accuracy: 0.5067\n",
      "Epoch 199/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.8546 - accuracy: 0.6700 - val_loss: 1.4492 - val_accuracy: 0.5233\n",
      "Epoch 200/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.8534 - accuracy: 0.6629 - val_loss: 1.4482 - val_accuracy: 0.5167\n",
      "Epoch 201/300\n",
      "70/70 [==============================] - 0s 829us/step - loss: 0.8520 - accuracy: 0.6586 - val_loss: 1.4623 - val_accuracy: 0.5100\n",
      "Epoch 202/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8513 - accuracy: 0.6700 - val_loss: 1.4595 - val_accuracy: 0.5100\n",
      "Epoch 203/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8495 - accuracy: 0.6671 - val_loss: 1.4588 - val_accuracy: 0.5033\n",
      "Epoch 204/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8483 - accuracy: 0.6700 - val_loss: 1.4585 - val_accuracy: 0.5167\n",
      "Epoch 205/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8471 - accuracy: 0.6729 - val_loss: 1.4594 - val_accuracy: 0.5033\n",
      "Epoch 206/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8449 - accuracy: 0.6800 - val_loss: 1.4640 - val_accuracy: 0.5133\n",
      "Epoch 207/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.8455 - accuracy: 0.6671 - val_loss: 1.4657 - val_accuracy: 0.5000\n",
      "Epoch 208/300\n",
      "70/70 [==============================] - 0s 843us/step - loss: 0.8436 - accuracy: 0.6786 - val_loss: 1.4691 - val_accuracy: 0.5067\n",
      "Epoch 209/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8420 - accuracy: 0.6700 - val_loss: 1.4549 - val_accuracy: 0.5167\n",
      "Epoch 210/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8414 - accuracy: 0.6686 - val_loss: 1.4662 - val_accuracy: 0.5200\n",
      "Epoch 211/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8398 - accuracy: 0.6757 - val_loss: 1.4690 - val_accuracy: 0.5267\n",
      "Epoch 212/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8374 - accuracy: 0.6686 - val_loss: 1.4674 - val_accuracy: 0.5033\n",
      "Epoch 213/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.8358 - accuracy: 0.6829 - val_loss: 1.4694 - val_accuracy: 0.4967\n",
      "Epoch 214/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8361 - accuracy: 0.6829 - val_loss: 1.4636 - val_accuracy: 0.5067\n",
      "Epoch 215/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8358 - accuracy: 0.6814 - val_loss: 1.4668 - val_accuracy: 0.5067\n",
      "Epoch 216/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.8340 - accuracy: 0.6729 - val_loss: 1.4754 - val_accuracy: 0.5133\n",
      "Epoch 217/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.8330 - accuracy: 0.6757 - val_loss: 1.4728 - val_accuracy: 0.5167\n",
      "Epoch 218/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8303 - accuracy: 0.6829 - val_loss: 1.4630 - val_accuracy: 0.5033\n",
      "Epoch 219/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8316 - accuracy: 0.6857 - val_loss: 1.4773 - val_accuracy: 0.5133\n",
      "Epoch 220/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.8295 - accuracy: 0.6757 - val_loss: 1.4743 - val_accuracy: 0.5100\n",
      "Epoch 221/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8278 - accuracy: 0.6943 - val_loss: 1.4850 - val_accuracy: 0.5067\n",
      "Epoch 222/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.8262 - accuracy: 0.6871 - val_loss: 1.4841 - val_accuracy: 0.5000\n",
      "Epoch 223/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8260 - accuracy: 0.6871 - val_loss: 1.4897 - val_accuracy: 0.5033\n",
      "Epoch 224/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8245 - accuracy: 0.7000 - val_loss: 1.4827 - val_accuracy: 0.5167\n",
      "Epoch 225/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8236 - accuracy: 0.6857 - val_loss: 1.4827 - val_accuracy: 0.4967\n",
      "Epoch 226/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8227 - accuracy: 0.6814 - val_loss: 1.4895 - val_accuracy: 0.5033\n",
      "Epoch 227/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 929us/step - loss: 0.8216 - accuracy: 0.6843 - val_loss: 1.4818 - val_accuracy: 0.5033\n",
      "Epoch 228/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8207 - accuracy: 0.6886 - val_loss: 1.4916 - val_accuracy: 0.5133\n",
      "Epoch 229/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8186 - accuracy: 0.6886 - val_loss: 1.4834 - val_accuracy: 0.5033\n",
      "Epoch 230/300\n",
      "70/70 [==============================] - 0s 972us/step - loss: 0.8179 - accuracy: 0.6829 - val_loss: 1.4912 - val_accuracy: 0.5167\n",
      "Epoch 231/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.8168 - accuracy: 0.6886 - val_loss: 1.4901 - val_accuracy: 0.5100\n",
      "Epoch 232/300\n",
      "70/70 [==============================] - 0s 843us/step - loss: 0.8159 - accuracy: 0.6971 - val_loss: 1.4966 - val_accuracy: 0.5200\n",
      "Epoch 233/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8150 - accuracy: 0.6886 - val_loss: 1.5032 - val_accuracy: 0.5000\n",
      "Epoch 234/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8130 - accuracy: 0.6929 - val_loss: 1.5034 - val_accuracy: 0.5000\n",
      "Epoch 235/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8139 - accuracy: 0.6943 - val_loss: 1.5047 - val_accuracy: 0.5033\n",
      "Epoch 236/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8118 - accuracy: 0.6843 - val_loss: 1.5113 - val_accuracy: 0.5067\n",
      "Epoch 237/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8096 - accuracy: 0.6886 - val_loss: 1.5012 - val_accuracy: 0.5100\n",
      "Epoch 238/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8096 - accuracy: 0.6957 - val_loss: 1.4954 - val_accuracy: 0.5100\n",
      "Epoch 239/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8088 - accuracy: 0.6971 - val_loss: 1.5144 - val_accuracy: 0.5033\n",
      "Epoch 240/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8078 - accuracy: 0.6929 - val_loss: 1.5149 - val_accuracy: 0.5133\n",
      "Epoch 241/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8055 - accuracy: 0.6886 - val_loss: 1.5181 - val_accuracy: 0.5000\n",
      "Epoch 242/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8060 - accuracy: 0.6914 - val_loss: 1.5104 - val_accuracy: 0.5000\n",
      "Epoch 243/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8037 - accuracy: 0.6943 - val_loss: 1.5062 - val_accuracy: 0.5133\n",
      "Epoch 244/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8050 - accuracy: 0.6829 - val_loss: 1.5051 - val_accuracy: 0.5133\n",
      "Epoch 245/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8040 - accuracy: 0.6957 - val_loss: 1.5151 - val_accuracy: 0.5000\n",
      "Epoch 246/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8024 - accuracy: 0.6943 - val_loss: 1.5291 - val_accuracy: 0.4900\n",
      "Epoch 247/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8015 - accuracy: 0.6986 - val_loss: 1.5251 - val_accuracy: 0.5033\n",
      "Epoch 248/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8002 - accuracy: 0.7000 - val_loss: 1.5142 - val_accuracy: 0.5067\n",
      "Epoch 249/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.7998 - accuracy: 0.6971 - val_loss: 1.5296 - val_accuracy: 0.4967\n",
      "Epoch 250/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.7996 - accuracy: 0.7043 - val_loss: 1.5117 - val_accuracy: 0.5067\n",
      "Epoch 251/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.7972 - accuracy: 0.6986 - val_loss: 1.5202 - val_accuracy: 0.5033\n",
      "Epoch 252/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.7970 - accuracy: 0.7000 - val_loss: 1.5237 - val_accuracy: 0.5067\n",
      "Epoch 253/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7947 - accuracy: 0.7057 - val_loss: 1.5294 - val_accuracy: 0.4967\n",
      "Epoch 254/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.7952 - accuracy: 0.6943 - val_loss: 1.5337 - val_accuracy: 0.4833\n",
      "Epoch 255/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7935 - accuracy: 0.7029 - val_loss: 1.5232 - val_accuracy: 0.5033\n",
      "Epoch 256/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.7922 - accuracy: 0.7000 - val_loss: 1.5487 - val_accuracy: 0.5000\n",
      "Epoch 257/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.7932 - accuracy: 0.7029 - val_loss: 1.5370 - val_accuracy: 0.5133\n",
      "Epoch 258/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7915 - accuracy: 0.7000 - val_loss: 1.5430 - val_accuracy: 0.4867\n",
      "Epoch 259/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.7897 - accuracy: 0.6971 - val_loss: 1.5350 - val_accuracy: 0.5033\n",
      "Epoch 260/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.7895 - accuracy: 0.7043 - val_loss: 1.5323 - val_accuracy: 0.5033\n",
      "Epoch 261/300\n",
      "70/70 [==============================] - 0s 843us/step - loss: 0.7886 - accuracy: 0.7071 - val_loss: 1.5449 - val_accuracy: 0.4967\n",
      "Epoch 262/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.7874 - accuracy: 0.7014 - val_loss: 1.5507 - val_accuracy: 0.4933\n",
      "Epoch 263/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.7877 - accuracy: 0.7029 - val_loss: 1.5446 - val_accuracy: 0.4967\n",
      "Epoch 264/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.7860 - accuracy: 0.7000 - val_loss: 1.5451 - val_accuracy: 0.4933\n",
      "Epoch 265/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.7858 - accuracy: 0.7071 - val_loss: 1.5513 - val_accuracy: 0.4867\n",
      "Epoch 266/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.7851 - accuracy: 0.7086 - val_loss: 1.5493 - val_accuracy: 0.4967\n",
      "Epoch 267/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.7832 - accuracy: 0.6986 - val_loss: 1.5395 - val_accuracy: 0.5067\n",
      "Epoch 268/300\n",
      "70/70 [==============================] - 0s 972us/step - loss: 0.7828 - accuracy: 0.7029 - val_loss: 1.5438 - val_accuracy: 0.5167\n",
      "Epoch 269/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.7824 - accuracy: 0.7029 - val_loss: 1.5346 - val_accuracy: 0.5033\n",
      "Epoch 270/300\n",
      "70/70 [==============================] - 0s 843us/step - loss: 0.7795 - accuracy: 0.7100 - val_loss: 1.5711 - val_accuracy: 0.4733\n",
      "Epoch 271/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.7816 - accuracy: 0.7100 - val_loss: 1.5491 - val_accuracy: 0.5067\n",
      "Epoch 272/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7792 - accuracy: 0.7029 - val_loss: 1.5411 - val_accuracy: 0.5033\n",
      "Epoch 273/300\n",
      "70/70 [==============================] - 0s 972us/step - loss: 0.7787 - accuracy: 0.7057 - val_loss: 1.5558 - val_accuracy: 0.5000\n",
      "Epoch 274/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.7753 - accuracy: 0.7100 - val_loss: 1.5826 - val_accuracy: 0.4800\n",
      "Epoch 275/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.7767 - accuracy: 0.7057 - val_loss: 1.5464 - val_accuracy: 0.5000\n",
      "Epoch 276/300\n",
      "70/70 [==============================] - 0s 872us/step - loss: 0.7760 - accuracy: 0.7129 - val_loss: 1.5758 - val_accuracy: 0.5000\n",
      "Epoch 277/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.7763 - accuracy: 0.6957 - val_loss: 1.5623 - val_accuracy: 0.4867\n",
      "Epoch 278/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7733 - accuracy: 0.7057 - val_loss: 1.5478 - val_accuracy: 0.5033\n",
      "Epoch 279/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.7736 - accuracy: 0.7100 - val_loss: 1.5657 - val_accuracy: 0.5033\n",
      "Epoch 280/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7720 - accuracy: 0.7100 - val_loss: 1.5643 - val_accuracy: 0.4900\n",
      "Epoch 281/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.7723 - accuracy: 0.7143 - val_loss: 1.5679 - val_accuracy: 0.5000\n",
      "Epoch 282/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7726 - accuracy: 0.7029 - val_loss: 1.5708 - val_accuracy: 0.4900\n",
      "Epoch 283/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.7706 - accuracy: 0.7057 - val_loss: 1.5777 - val_accuracy: 0.4933\n",
      "Epoch 284/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7708 - accuracy: 0.7100 - val_loss: 1.5609 - val_accuracy: 0.5067\n",
      "Epoch 285/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.7703 - accuracy: 0.7086 - val_loss: 1.5647 - val_accuracy: 0.4967\n",
      "Epoch 286/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.7665 - accuracy: 0.7143 - val_loss: 1.5870 - val_accuracy: 0.4833\n",
      "Epoch 287/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.7677 - accuracy: 0.7071 - val_loss: 1.5870 - val_accuracy: 0.4867\n",
      "Epoch 288/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7665 - accuracy: 0.7071 - val_loss: 1.5732 - val_accuracy: 0.4967\n",
      "Epoch 289/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.7664 - accuracy: 0.7086 - val_loss: 1.5780 - val_accuracy: 0.4867\n",
      "Epoch 290/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.7662 - accuracy: 0.7143 - val_loss: 1.5765 - val_accuracy: 0.4967\n",
      "Epoch 291/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.7639 - accuracy: 0.7171 - val_loss: 1.5750 - val_accuracy: 0.4967\n",
      "Epoch 292/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7640 - accuracy: 0.7129 - val_loss: 1.5708 - val_accuracy: 0.5033\n",
      "Epoch 293/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.7640 - accuracy: 0.7086 - val_loss: 1.5829 - val_accuracy: 0.4967\n",
      "Epoch 294/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7619 - accuracy: 0.7157 - val_loss: 1.5809 - val_accuracy: 0.5033\n",
      "Epoch 295/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.7615 - accuracy: 0.7071 - val_loss: 1.5809 - val_accuracy: 0.4933\n",
      "Epoch 296/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.7600 - accuracy: 0.7100 - val_loss: 1.5799 - val_accuracy: 0.4933\n",
      "Epoch 297/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.7608 - accuracy: 0.7086 - val_loss: 1.5856 - val_accuracy: 0.4900\n",
      "Epoch 298/300\n",
      "70/70 [==============================] - 0s 843us/step - loss: 0.7588 - accuracy: 0.7100 - val_loss: 1.5955 - val_accuracy: 0.4933\n",
      "Epoch 299/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7576 - accuracy: 0.7229 - val_loss: 1.5853 - val_accuracy: 0.4933\n",
      "Epoch 300/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.7577 - accuracy: 0.7214 - val_loss: 1.5997 - val_accuracy: 0.4967\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(X_train, Y_train, epochs=300, batch_size=10, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 844us/step - loss: 1.5811 - accuracy: 0.4938\n",
      "\n",
      "loss : 1.5811048746109009\n",
      "accuray : 0.49380001425743103\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "print('')\n",
    "print('loss : ' + str(loss_and_metrics[0]))\n",
    "print('accuray : ' + str(loss_and_metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "es=EarlyStopping(patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 1/70 [..............................] - ETA: 0s - loss: 2.3536 - accuracy: 0.3000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.2752 - accuracy: 0.1486 - val_loss: 2.2188 - val_accuracy: 0.1833\n",
      "Epoch 2/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 2.1949 - accuracy: 0.1929 - val_loss: 2.1394 - val_accuracy: 0.1700\n",
      "Epoch 3/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 2.1104 - accuracy: 0.2143 - val_loss: 2.0648 - val_accuracy: 0.2233\n",
      "Epoch 4/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 2.0397 - accuracy: 0.2557 - val_loss: 1.9992 - val_accuracy: 0.2700\n",
      "Epoch 5/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.9801 - accuracy: 0.2729 - val_loss: 1.9429 - val_accuracy: 0.2833\n",
      "Epoch 6/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.9290 - accuracy: 0.2714 - val_loss: 1.8943 - val_accuracy: 0.2900\n",
      "Epoch 7/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.8814 - accuracy: 0.2657 - val_loss: 1.8472 - val_accuracy: 0.2700\n",
      "Epoch 8/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.8421 - accuracy: 0.2686 - val_loss: 1.8094 - val_accuracy: 0.2667\n",
      "Epoch 9/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.8048 - accuracy: 0.2700 - val_loss: 1.7783 - val_accuracy: 0.2733\n",
      "Epoch 10/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.7732 - accuracy: 0.2771 - val_loss: 1.7507 - val_accuracy: 0.2767\n",
      "Epoch 11/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.7473 - accuracy: 0.2829 - val_loss: 1.7259 - val_accuracy: 0.2800\n",
      "Epoch 12/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.7223 - accuracy: 0.3029 - val_loss: 1.7024 - val_accuracy: 0.2967\n",
      "Epoch 13/500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.7000 - accuracy: 0.3057 - val_loss: 1.6908 - val_accuracy: 0.3233\n",
      "Epoch 14/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.6822 - accuracy: 0.3157 - val_loss: 1.6710 - val_accuracy: 0.3167\n",
      "Epoch 15/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.6630 - accuracy: 0.3171 - val_loss: 1.6614 - val_accuracy: 0.3233\n",
      "Epoch 16/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.6459 - accuracy: 0.3371 - val_loss: 1.6486 - val_accuracy: 0.3233\n",
      "Epoch 17/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.6313 - accuracy: 0.3243 - val_loss: 1.6319 - val_accuracy: 0.3333\n",
      "Epoch 18/500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.6162 - accuracy: 0.3357 - val_loss: 1.6210 - val_accuracy: 0.3567\n",
      "Epoch 19/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.6006 - accuracy: 0.3543 - val_loss: 1.6114 - val_accuracy: 0.3500\n",
      "Epoch 20/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.5880 - accuracy: 0.3457 - val_loss: 1.6011 - val_accuracy: 0.3733\n",
      "Epoch 21/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.5753 - accuracy: 0.3543 - val_loss: 1.5895 - val_accuracy: 0.3633\n",
      "Epoch 22/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5620 - accuracy: 0.3714 - val_loss: 1.5900 - val_accuracy: 0.3267\n",
      "Epoch 23/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5500 - accuracy: 0.3671 - val_loss: 1.5747 - val_accuracy: 0.3533\n",
      "Epoch 24/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.5398 - accuracy: 0.3914 - val_loss: 1.5685 - val_accuracy: 0.3600\n",
      "Epoch 25/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5287 - accuracy: 0.3700 - val_loss: 1.5630 - val_accuracy: 0.3567\n",
      "Epoch 26/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.5180 - accuracy: 0.3629 - val_loss: 1.5527 - val_accuracy: 0.3533\n",
      "Epoch 27/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5083 - accuracy: 0.3771 - val_loss: 1.5555 - val_accuracy: 0.3633\n",
      "Epoch 28/500\n",
      "70/70 [==============================] - 0s 829us/step - loss: 1.4978 - accuracy: 0.3929 - val_loss: 1.5489 - val_accuracy: 0.3600\n",
      "Epoch 29/500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.4887 - accuracy: 0.3900 - val_loss: 1.5416 - val_accuracy: 0.3600\n",
      "Epoch 30/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.4796 - accuracy: 0.4057 - val_loss: 1.5362 - val_accuracy: 0.3533\n",
      "Epoch 31/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.4707 - accuracy: 0.4029 - val_loss: 1.5337 - val_accuracy: 0.3667\n",
      "Epoch 32/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.4617 - accuracy: 0.4086 - val_loss: 1.5248 - val_accuracy: 0.3667\n",
      "Epoch 33/500\n",
      "70/70 [==============================] - 0s 972us/step - loss: 1.4534 - accuracy: 0.4243 - val_loss: 1.5256 - val_accuracy: 0.3700\n",
      "Epoch 34/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.4454 - accuracy: 0.4114 - val_loss: 1.5251 - val_accuracy: 0.3800\n",
      "Epoch 35/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.4385 - accuracy: 0.4200 - val_loss: 1.5193 - val_accuracy: 0.3533\n",
      "Epoch 36/500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.4313 - accuracy: 0.4157 - val_loss: 1.5142 - val_accuracy: 0.3600\n",
      "Epoch 37/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.4228 - accuracy: 0.4043 - val_loss: 1.5068 - val_accuracy: 0.3567\n",
      "Epoch 38/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.4157 - accuracy: 0.4300 - val_loss: 1.5117 - val_accuracy: 0.3700\n",
      "Epoch 39/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.4093 - accuracy: 0.4157 - val_loss: 1.5073 - val_accuracy: 0.3733\n",
      "Epoch 40/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.4017 - accuracy: 0.4214 - val_loss: 1.4993 - val_accuracy: 0.3600\n",
      "Epoch 41/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.3949 - accuracy: 0.4243 - val_loss: 1.4908 - val_accuracy: 0.3500\n",
      "Epoch 42/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.3894 - accuracy: 0.4214 - val_loss: 1.4948 - val_accuracy: 0.3533\n",
      "Epoch 43/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3830 - accuracy: 0.4400 - val_loss: 1.4955 - val_accuracy: 0.3567\n",
      "Epoch 44/500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.3759 - accuracy: 0.4343 - val_loss: 1.4959 - val_accuracy: 0.3633\n",
      "Epoch 45/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3694 - accuracy: 0.4314 - val_loss: 1.5000 - val_accuracy: 0.3733\n",
      "Epoch 46/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.3662 - accuracy: 0.4400 - val_loss: 1.4897 - val_accuracy: 0.3767\n",
      "Epoch 47/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.3599 - accuracy: 0.4414 - val_loss: 1.4866 - val_accuracy: 0.3567\n",
      "Epoch 48/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.3543 - accuracy: 0.4471 - val_loss: 1.4823 - val_accuracy: 0.3667\n",
      "Epoch 49/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3489 - accuracy: 0.4529 - val_loss: 1.4871 - val_accuracy: 0.3767\n",
      "Epoch 50/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3415 - accuracy: 0.4486 - val_loss: 1.4886 - val_accuracy: 0.3767\n",
      "Epoch 51/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3370 - accuracy: 0.4443 - val_loss: 1.4827 - val_accuracy: 0.3600\n",
      "Epoch 52/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.3321 - accuracy: 0.4443 - val_loss: 1.4939 - val_accuracy: 0.3767\n",
      "Epoch 53/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3281 - accuracy: 0.4486 - val_loss: 1.4809 - val_accuracy: 0.3700\n",
      "Epoch 54/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3206 - accuracy: 0.4729 - val_loss: 1.4814 - val_accuracy: 0.3600\n",
      "Epoch 55/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3153 - accuracy: 0.4486 - val_loss: 1.4782 - val_accuracy: 0.3700\n",
      "Epoch 56/500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.3114 - accuracy: 0.4571 - val_loss: 1.4781 - val_accuracy: 0.3733\n",
      "Epoch 57/500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.3074 - accuracy: 0.4571 - val_loss: 1.4798 - val_accuracy: 0.3700\n",
      "Epoch 58/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.3012 - accuracy: 0.4629 - val_loss: 1.4712 - val_accuracy: 0.3733\n",
      "Epoch 59/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.2968 - accuracy: 0.4757 - val_loss: 1.4686 - val_accuracy: 0.3700\n",
      "Epoch 60/500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.2929 - accuracy: 0.4686 - val_loss: 1.4702 - val_accuracy: 0.3933\n",
      "Epoch 61/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.2889 - accuracy: 0.4786 - val_loss: 1.4799 - val_accuracy: 0.3900\n",
      "Epoch 62/500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.2842 - accuracy: 0.4829 - val_loss: 1.4734 - val_accuracy: 0.3900\n",
      "Epoch 63/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.2795 - accuracy: 0.4929 - val_loss: 1.4662 - val_accuracy: 0.3800\n",
      "Epoch 64/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.2741 - accuracy: 0.4814 - val_loss: 1.4800 - val_accuracy: 0.3900\n",
      "Epoch 65/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.2714 - accuracy: 0.4857 - val_loss: 1.4767 - val_accuracy: 0.3933\n",
      "Epoch 66/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.2670 - accuracy: 0.4900 - val_loss: 1.4701 - val_accuracy: 0.3967\n",
      "Epoch 67/500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.2635 - accuracy: 0.4829 - val_loss: 1.4629 - val_accuracy: 0.3967\n",
      "Epoch 68/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.2589 - accuracy: 0.4857 - val_loss: 1.4598 - val_accuracy: 0.3700\n",
      "Epoch 69/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.2508 - accuracy: 0.4800 - val_loss: 1.4726 - val_accuracy: 0.4267\n",
      "Epoch 70/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.2505 - accuracy: 0.4943 - val_loss: 1.4604 - val_accuracy: 0.4100\n",
      "Epoch 71/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.2473 - accuracy: 0.5043 - val_loss: 1.4611 - val_accuracy: 0.4133\n",
      "Epoch 72/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.2416 - accuracy: 0.5043 - val_loss: 1.4546 - val_accuracy: 0.4033\n",
      "Epoch 73/500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.2373 - accuracy: 0.5000 - val_loss: 1.4577 - val_accuracy: 0.3867\n",
      "Epoch 74/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.2321 - accuracy: 0.5143 - val_loss: 1.4671 - val_accuracy: 0.4167\n",
      "Epoch 75/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.2277 - accuracy: 0.5100 - val_loss: 1.4448 - val_accuracy: 0.3967\n",
      "Epoch 76/500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.2236 - accuracy: 0.5043 - val_loss: 1.4672 - val_accuracy: 0.4067\n",
      "Epoch 77/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.2215 - accuracy: 0.5186 - val_loss: 1.4604 - val_accuracy: 0.4467\n",
      "Epoch 78/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.2159 - accuracy: 0.5071 - val_loss: 1.4435 - val_accuracy: 0.4133\n",
      "Epoch 79/500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.2118 - accuracy: 0.5243 - val_loss: 1.4410 - val_accuracy: 0.4267\n",
      "Epoch 80/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.2063 - accuracy: 0.5271 - val_loss: 1.4509 - val_accuracy: 0.4467\n",
      "Epoch 81/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.2040 - accuracy: 0.5329 - val_loss: 1.4329 - val_accuracy: 0.4300\n",
      "Epoch 82/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1969 - accuracy: 0.5257 - val_loss: 1.4526 - val_accuracy: 0.4400\n",
      "Epoch 83/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1940 - accuracy: 0.5300 - val_loss: 1.4407 - val_accuracy: 0.4067\n",
      "Epoch 84/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1881 - accuracy: 0.5300 - val_loss: 1.4283 - val_accuracy: 0.4300\n",
      "Epoch 85/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1874 - accuracy: 0.5557 - val_loss: 1.4292 - val_accuracy: 0.4367\n",
      "Epoch 86/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1815 - accuracy: 0.5486 - val_loss: 1.4282 - val_accuracy: 0.4233\n",
      "Epoch 87/500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.1774 - accuracy: 0.5443 - val_loss: 1.4231 - val_accuracy: 0.4400\n",
      "Epoch 88/500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.1740 - accuracy: 0.5386 - val_loss: 1.4393 - val_accuracy: 0.4333\n",
      "Epoch 89/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1702 - accuracy: 0.5486 - val_loss: 1.4243 - val_accuracy: 0.4367\n",
      "Epoch 90/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1675 - accuracy: 0.5471 - val_loss: 1.4330 - val_accuracy: 0.4500\n",
      "Epoch 91/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1603 - accuracy: 0.5529 - val_loss: 1.4177 - val_accuracy: 0.4367\n",
      "Epoch 92/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1572 - accuracy: 0.5486 - val_loss: 1.4200 - val_accuracy: 0.4500\n",
      "Epoch 93/500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.1506 - accuracy: 0.5457 - val_loss: 1.4224 - val_accuracy: 0.4733\n",
      "Epoch 94/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1486 - accuracy: 0.5657 - val_loss: 1.4185 - val_accuracy: 0.4300\n",
      "Epoch 95/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1431 - accuracy: 0.5514 - val_loss: 1.4027 - val_accuracy: 0.4467\n",
      "Epoch 96/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.1385 - accuracy: 0.5743 - val_loss: 1.3974 - val_accuracy: 0.4233\n",
      "Epoch 97/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1348 - accuracy: 0.5671 - val_loss: 1.4078 - val_accuracy: 0.4600\n",
      "Epoch 98/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.1283 - accuracy: 0.5757 - val_loss: 1.3984 - val_accuracy: 0.4533\n",
      "Epoch 99/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.1240 - accuracy: 0.5686 - val_loss: 1.4003 - val_accuracy: 0.4333\n",
      "Epoch 100/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1224 - accuracy: 0.5800 - val_loss: 1.4066 - val_accuracy: 0.4600\n",
      "Epoch 101/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1161 - accuracy: 0.5671 - val_loss: 1.3989 - val_accuracy: 0.4633\n",
      "Epoch 102/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.1124 - accuracy: 0.5786 - val_loss: 1.4044 - val_accuracy: 0.4633\n",
      "Epoch 103/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1059 - accuracy: 0.5757 - val_loss: 1.3888 - val_accuracy: 0.4667\n",
      "Epoch 104/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.1053 - accuracy: 0.5929 - val_loss: 1.3945 - val_accuracy: 0.4600\n",
      "Epoch 105/500\n",
      "70/70 [==============================] - 0s 972us/step - loss: 1.1018 - accuracy: 0.5771 - val_loss: 1.4030 - val_accuracy: 0.4733\n",
      "Epoch 106/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0957 - accuracy: 0.5714 - val_loss: 1.3999 - val_accuracy: 0.4867\n",
      "Epoch 107/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0939 - accuracy: 0.6057 - val_loss: 1.3792 - val_accuracy: 0.4800\n",
      "Epoch 108/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.0901 - accuracy: 0.5857 - val_loss: 1.3874 - val_accuracy: 0.4667\n",
      "Epoch 109/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0868 - accuracy: 0.6029 - val_loss: 1.3864 - val_accuracy: 0.4967\n",
      "Epoch 110/500\n",
      "70/70 [==============================] - 0s 1000us/step - loss: 1.0802 - accuracy: 0.5886 - val_loss: 1.3904 - val_accuracy: 0.4967\n",
      "Epoch 111/500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.0799 - accuracy: 0.6014 - val_loss: 1.3927 - val_accuracy: 0.5233\n",
      "Epoch 112/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0778 - accuracy: 0.5971 - val_loss: 1.3981 - val_accuracy: 0.4967\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0732 - accuracy: 0.6157 - val_loss: 1.3993 - val_accuracy: 0.4867\n",
      "Epoch 114/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0694 - accuracy: 0.6014 - val_loss: 1.3872 - val_accuracy: 0.4867\n",
      "Epoch 115/500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.0658 - accuracy: 0.6100 - val_loss: 1.3725 - val_accuracy: 0.5100\n",
      "Epoch 116/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.0639 - accuracy: 0.6129 - val_loss: 1.3873 - val_accuracy: 0.4967\n",
      "Epoch 117/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0594 - accuracy: 0.6157 - val_loss: 1.3705 - val_accuracy: 0.5133\n",
      "Epoch 118/500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.0553 - accuracy: 0.6100 - val_loss: 1.3747 - val_accuracy: 0.5200\n",
      "Epoch 119/500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.0519 - accuracy: 0.6129 - val_loss: 1.3802 - val_accuracy: 0.5067\n",
      "Epoch 120/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.0484 - accuracy: 0.6143 - val_loss: 1.3813 - val_accuracy: 0.4967\n",
      "Epoch 121/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0462 - accuracy: 0.6143 - val_loss: 1.3687 - val_accuracy: 0.5233\n",
      "Epoch 122/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0423 - accuracy: 0.6171 - val_loss: 1.3708 - val_accuracy: 0.5267\n",
      "Epoch 123/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0406 - accuracy: 0.6129 - val_loss: 1.3811 - val_accuracy: 0.5233\n",
      "Epoch 124/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.0354 - accuracy: 0.6214 - val_loss: 1.3689 - val_accuracy: 0.5067\n",
      "Epoch 125/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0327 - accuracy: 0.6157 - val_loss: 1.3788 - val_accuracy: 0.5100\n",
      "Epoch 126/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0286 - accuracy: 0.6200 - val_loss: 1.3765 - val_accuracy: 0.5133\n",
      "Epoch 127/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0285 - accuracy: 0.6186 - val_loss: 1.3733 - val_accuracy: 0.5100\n",
      "Epoch 128/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0239 - accuracy: 0.6129 - val_loss: 1.3826 - val_accuracy: 0.5200\n",
      "Epoch 129/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0217 - accuracy: 0.6229 - val_loss: 1.3724 - val_accuracy: 0.5000\n",
      "Epoch 130/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0163 - accuracy: 0.6300 - val_loss: 1.3819 - val_accuracy: 0.5000\n",
      "Epoch 131/500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.0166 - accuracy: 0.6200 - val_loss: 1.3790 - val_accuracy: 0.5067\n",
      "Epoch 132/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0113 - accuracy: 0.6300 - val_loss: 1.3776 - val_accuracy: 0.5033\n",
      "Epoch 133/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0110 - accuracy: 0.6200 - val_loss: 1.3552 - val_accuracy: 0.5267\n",
      "Epoch 134/500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.0067 - accuracy: 0.6214 - val_loss: 1.3758 - val_accuracy: 0.5200\n",
      "Epoch 135/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0015 - accuracy: 0.6286 - val_loss: 1.3635 - val_accuracy: 0.5067\n",
      "Epoch 136/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0007 - accuracy: 0.6171 - val_loss: 1.3682 - val_accuracy: 0.5067\n",
      "Epoch 137/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9998 - accuracy: 0.6200 - val_loss: 1.3630 - val_accuracy: 0.5167\n",
      "Epoch 138/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9967 - accuracy: 0.6214 - val_loss: 1.3697 - val_accuracy: 0.5300\n",
      "Epoch 139/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9934 - accuracy: 0.6271 - val_loss: 1.3578 - val_accuracy: 0.5033\n",
      "Epoch 140/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9904 - accuracy: 0.6214 - val_loss: 1.3472 - val_accuracy: 0.5067\n",
      "Epoch 141/500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.9870 - accuracy: 0.6300 - val_loss: 1.3779 - val_accuracy: 0.5200\n",
      "Epoch 142/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9860 - accuracy: 0.6343 - val_loss: 1.3669 - val_accuracy: 0.4933\n",
      "Epoch 143/500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.9834 - accuracy: 0.6257 - val_loss: 1.3590 - val_accuracy: 0.4967\n",
      "Epoch 144/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.9792 - accuracy: 0.6243 - val_loss: 1.3684 - val_accuracy: 0.5233\n",
      "Epoch 145/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.9776 - accuracy: 0.6271 - val_loss: 1.3617 - val_accuracy: 0.5133\n",
      "Epoch 146/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.9742 - accuracy: 0.6314 - val_loss: 1.3695 - val_accuracy: 0.5100\n",
      "Epoch 147/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9729 - accuracy: 0.6314 - val_loss: 1.3555 - val_accuracy: 0.5200\n",
      "Epoch 148/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9691 - accuracy: 0.6314 - val_loss: 1.3750 - val_accuracy: 0.4933\n",
      "Epoch 149/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.9695 - accuracy: 0.6200 - val_loss: 1.3683 - val_accuracy: 0.5067\n",
      "Epoch 150/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9658 - accuracy: 0.6357 - val_loss: 1.3553 - val_accuracy: 0.4967\n",
      "Epoch 151/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9630 - accuracy: 0.6329 - val_loss: 1.3502 - val_accuracy: 0.5067\n",
      "Epoch 152/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9622 - accuracy: 0.6386 - val_loss: 1.3636 - val_accuracy: 0.5033\n",
      "Epoch 153/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9595 - accuracy: 0.6443 - val_loss: 1.3582 - val_accuracy: 0.5100\n",
      "Epoch 154/500\n",
      "70/70 [==============================] - 0s 972us/step - loss: 0.9595 - accuracy: 0.6286 - val_loss: 1.3694 - val_accuracy: 0.5133\n",
      "Epoch 155/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.9518 - accuracy: 0.6443 - val_loss: 1.3657 - val_accuracy: 0.5033\n",
      "Epoch 156/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9509 - accuracy: 0.6357 - val_loss: 1.3543 - val_accuracy: 0.5033\n",
      "Epoch 157/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9496 - accuracy: 0.6157 - val_loss: 1.3692 - val_accuracy: 0.5100\n",
      "Epoch 158/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9473 - accuracy: 0.6314 - val_loss: 1.3611 - val_accuracy: 0.5033\n",
      "Epoch 159/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.9435 - accuracy: 0.6314 - val_loss: 1.3541 - val_accuracy: 0.5300\n",
      "Epoch 160/500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.9426 - accuracy: 0.6271 - val_loss: 1.3580 - val_accuracy: 0.5233\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(X_train, Y_train, epochs=500, batch_size=10, validation_data=(X_val, Y_val), \n",
    "               callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 1.1194 - accuracy: 0.7276 - val_loss: 0.6548 - val_accuracy: 0.8369\n",
      "Epoch 2/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.5209 - accuracy: 0.8695 - val_loss: 0.4813 - val_accuracy: 0.8705\n",
      "Epoch 3/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.4179 - accuracy: 0.8889 - val_loss: 0.4203 - val_accuracy: 0.8819\n",
      "Epoch 4/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3725 - accuracy: 0.8973 - val_loss: 0.3857 - val_accuracy: 0.8922\n",
      "Epoch 5/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3434 - accuracy: 0.9051 - val_loss: 0.3642 - val_accuracy: 0.8970\n",
      "313/313 [==============================] - 0s 649us/step - loss: 0.3393 - accuracy: 0.9044\n",
      "\n",
      "loss_and_metrics : [0.33927416801452637, 0.9043999910354614]\n",
      "WARNING:tensorflow:From <ipython-input-19-cc256cab72d0>:47: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "True : 9, Predict : 9\n",
      "True : 9, Predict : 9\n",
      "True : 2, Predict : 2\n",
      "True : 1, Predict : 1\n",
      "True : 9, Predict : 9\n"
     ]
    }
   ],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 데이터셋 전처리\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 원핫인코딩 (one-hot encoding) 처리\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[:42000] # 훈련셋의 30%를 검증셋으로 사용\n",
    "x_train = x_train[42000:]\n",
    "y_val = y_train[:42000] # 훈련셋의 30%를 검증셋으로 사용\n",
    "y_train = y_train[42000:]\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('')\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
    "\n",
    "# 6. 모델 사용하기\n",
    "xhat_idx = np.random.choice(x_test.shape[0], 5)\n",
    "xhat = x_test[xhat_idx]\n",
    "yhat = model.predict_classes(xhat)\n",
    "\n",
    "for i in range(5):\n",
    "    print('True : ' + str(argmax(y_test[xhat_idx[i]])) + ', Predict : ' + str(yhat[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mymnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "xhat_idx = np.random.choice(x_test.shape[0], 5)\n",
    "xhat = x_test[xhat_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(\"mymnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat=model.predict_classes(xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True : 3, Predict : 3\n",
      "True : 5, Predict : 5\n",
      "True : 7, Predict : 7\n",
      "True : 0, Predict : 0\n",
      "True : 6, Predict : 6\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('True : ' + str(argmax(y_test[xhat_idx[i]])) + ', Predict : ' + str(yhat[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 이용한 수열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM, Embedding, SimpleRNN\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([SimpleRNN(units=1, activation='tanh', return_sequences=False,\n",
    "                     return_state=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. ]\n",
      "  [0.1]\n",
      "  [0.2]\n",
      "  [0.3]]\n",
      "\n",
      " [[0.1]\n",
      "  [0.2]\n",
      "  [0.3]\n",
      "  [0.4]]\n",
      "\n",
      " [[0.2]\n",
      "  [0.3]\n",
      "  [0.4]\n",
      "  [0.5]]\n",
      "\n",
      " [[0.3]\n",
      "  [0.4]\n",
      "  [0.5]\n",
      "  [0.6]]\n",
      "\n",
      " [[0.4]\n",
      "  [0.5]\n",
      "  [0.6]\n",
      "  [0.7]]\n",
      "\n",
      " [[0.5]\n",
      "  [0.6]\n",
      "  [0.7]\n",
      "  [0.8]]]\n",
      "[0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    }
   ],
   "source": [
    "X = [] \n",
    "Y = [] \n",
    "for i in range(6): \n",
    "    lst = list(range(i,i+4)) \n",
    "    X.append(list(map(lambda c: [c/10], lst))) \n",
    "    Y.append((i+4)/10) \n",
    "X = np.array(X) \n",
    "Y = np.array(Y) \n",
    "print(X)  #6,4,1\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([SimpleRNN(units=10, return_sequences=False, input_shape=[4,1]), Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5520\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.5327\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5139\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4955\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4775\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4600\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 993us/step - loss: 0.4429\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.4262\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.4099\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3941\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.3787\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3637\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3491\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3349\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3210\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3076\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2945\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2818\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2695\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2575\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2459\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2346\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.2237\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2130\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2027\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1928\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1831\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.1737\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.1647\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1559\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.1474\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1393\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1314\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1238\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1165\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1094\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1026\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0961\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0899\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0839\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0782\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0728\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0676\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0627\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0580\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0535\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0493\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0453\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0416\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0381\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0348\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0317\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0289\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0262\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0237\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0215\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0194\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0157\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0114\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0103\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0083\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0075\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0068\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0062\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0057\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0053\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0049\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0046\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0043\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0041\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0039\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0038\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0037\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0036\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0035\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0035\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0034\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0034\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0034\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0034\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0034\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0034\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0033\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0033\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0033\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0033\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0031\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0031\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0030\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0030\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X, Y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4344244 ],\n",
       "       [0.56301427],\n",
       "       [0.6607542 ],\n",
       "       [0.73152626],\n",
       "       [0.7811376 ],\n",
       "       [0.8148129 ]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8481271]]\n"
     ]
    }
   ],
   "source": [
    "sooneung=np.array([[[1.0],[1.1],[1.2],[1.3]]])\n",
    "print(model.predict(sooneung))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 기반 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"인공지능을 공부하면서 코딩을 하고 있다\\n\n",
    "파이썬 코딩을 배우고 익혔다\\n\n",
    "딥러닝을 배우고 코딩을 하고 있다\\n\n",
    "파이썬 기반에서 판다스를 배우고 코딩을 했다\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'코딩을': 1,\n",
       " '배우고': 2,\n",
       " '하고': 3,\n",
       " '있다': 4,\n",
       " '파이썬': 5,\n",
       " '인공지능을': 6,\n",
       " '공부하면서': 7,\n",
       " '익혔다': 8,\n",
       " '딥러닝을': 9,\n",
       " '기반에서': 10,\n",
       " '판다스를': 11,\n",
       " '했다': 12}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('인공지능을', 1),\n",
       "             ('공부하면서', 1),\n",
       "             ('코딩을', 4),\n",
       "             ('하고', 2),\n",
       "             ('있다', 2),\n",
       "             ('파이썬', 2),\n",
       "             ('배우고', 3),\n",
       "             ('익혔다', 1),\n",
       "             ('딥러닝을', 1),\n",
       "             ('기반에서', 1),\n",
       "             ('판다스를', 1),\n",
       "             ('했다', 1)])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'): \n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 7],\n",
       " [6, 7, 1],\n",
       " [6, 7, 1, 3],\n",
       " [6, 7, 1, 3, 4],\n",
       " [5, 1],\n",
       " [5, 1, 2],\n",
       " [5, 1, 2, 8],\n",
       " [9, 2],\n",
       " [9, 2, 1],\n",
       " [9, 2, 1, 3],\n",
       " [9, 2, 1, 3, 4],\n",
       " [5, 10],\n",
       " [5, 10, 11],\n",
       " [5, 10, 11, 2],\n",
       " [5, 10, 11, 2, 1],\n",
       " [5, 10, 11, 2, 1, 12]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=max(len(i) for i in sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=pad_sequences(sequences, maxlen=max_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sequences[:,:-1]\n",
    "y=sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  1,  3,  4,  1,  2,  8,  2,  1,  3,  4, 10, 11,  2,  1, 12])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential(Embedding(vocab_size, 10, input_length=max_len-1)) \n",
    "# 고차원(vocab_size, sparse_matrix)->저차원(dense_matrix) : Embedding\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:748 train_step\n        loss = self.compiled_loss(\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 1) and (None, 13) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-b9624e2c9487>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:748 train_step\n        loss = self.compiled_loss(\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    C:\\Users\\i\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 1) and (None, 13) are incompatible\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n):\n",
    "    init_word=current_word\n",
    "    sentence=''\n",
    "    for _ in range(n):\n",
    "        encoded=t.texts_to_sequences([current_word])[0] # 단어('인공지능을')의 인덱스\n",
    "        encoded=pad_sequences([encoded], maxlen=5, padding='pre')\n",
    "        result=model.predict_classes(encoded)\n",
    "        for word, index in t.word_index.items():\n",
    "            if index==result:\n",
    "                break\n",
    "        current_word=current_word+\" \"+word\n",
    "        sentence=sentence+\" \"+word\n",
    "    sentence=init_word+sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_generation(model, t, \"인공지능을\", 4)) # 출력 : 인공지능을 공부하면서 코딩을 하고 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.word_index\n",
    "t.index_word\n",
    "print(t.texts_to_sequences(['인공지능을 공부하면서']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
