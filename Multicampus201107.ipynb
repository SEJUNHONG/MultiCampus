{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[50000:]\n",
    "Y_val = Y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(50000, 784).astype('float32') / 255.0\n",
    "X_val = X_val.reshape(10000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rand_idxs = np.random.choice(50000, 700)\n",
    "val_rand_idxs = np.random.choice(10000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5994,  1688, 11513, 48056, 25544, 39168, 25365, 11261, 19091,\n",
       "       30410, 43883, 23545, 39829,  1447,  6542, 19960,  3610, 34266,\n",
       "        2710, 25410,  8127, 44348, 12929,  4019,   858, 24133,  7016,\n",
       "        4718, 30305,  6557, 42602, 31896, 47611, 37694, 10759, 31736,\n",
       "       11435,  7713, 22907, 46415, 20656,  7188, 15198,  5525, 33151,\n",
       "        8055, 22070, 17408,  8896,  8466, 17780, 14271, 16037, 35000,\n",
       "       30648, 42749, 16046, 33313,  2161, 11265,  8021, 45173, 45943,\n",
       "       30666, 16107, 40828, 19428,  2779, 23039, 43920, 37874, 43282,\n",
       "       40779, 34999, 41824,  7391, 20621, 17822, 36400, 14269, 31028,\n",
       "       43505, 29180, 17138, 34818, 38044, 22564, 49836, 47280, 30523,\n",
       "        8374,  1520, 16868, 28251, 12565, 21105, 18232, 40552,  8487,\n",
       "        6429,  1712, 25609, 23585,  9711,  2492, 27959, 37131,  3284,\n",
       "       43914, 37232, 42704, 43084,  4932, 19756, 26668,  1019, 48016,\n",
       "       28327, 42162,  6081, 26659, 23725, 24818, 28724, 36097, 26386,\n",
       "       16319, 39938, 24279, 41443,  9364, 24529,  7190, 17387,  8839,\n",
       "       13793,  5035,  6688,  9820, 23356, 46992,  4891, 31029,   363,\n",
       "       23048,  4884, 45832,  5784, 13262,  7756, 18532, 40841, 25266,\n",
       "       18492, 49285, 14548, 12791, 16873, 42602, 15697, 31753, 40077,\n",
       "       34302, 28716, 38471, 17244, 13604, 24490, 35016, 15508, 25662,\n",
       "       22700, 36510,  8849, 49524, 12320, 22197, 10791,    32, 48599,\n",
       "       40316, 35157, 27955, 25690, 14543, 14684, 26379, 18166, 32233,\n",
       "       45073,  8627,  6157, 21087, 46020,  1023,  6207, 23849,  4045,\n",
       "       18016, 49880, 13372, 40879, 32164, 19127,  2282, 30392, 24277,\n",
       "       19799, 25052, 31644, 20847, 36462,  2955, 28707, 37077,  6364,\n",
       "       15153, 23624, 41177, 27682, 13579, 42488, 40409, 15035, 37764,\n",
       "       20053,  8120, 23646, 18093, 41825, 30869,  9378, 22266,  7709,\n",
       "       49578, 29447, 28382, 42316, 33959, 28473, 18677, 18284, 24236,\n",
       "       24853, 41513, 41692,  1058, 33423, 34605, 12580, 44968, 13914,\n",
       "       14900,  6120, 10565, 37302, 14220, 10266,  6580, 30434, 37163,\n",
       "       19319, 36503, 29106,  6464,  1973, 10874, 36819,  4378, 20739,\n",
       "         277, 25198, 36340, 19066, 16279,  5613, 31564, 30482, 31522,\n",
       "       21340, 33427, 46396, 48502, 48871,   767,  4717, 46631,  4672,\n",
       "       34364, 42892,  1960, 44921, 13114,  5854, 19861, 19180, 47360,\n",
       "       27171, 28749,  9237,  8772, 13282, 35909, 42353,  4071, 40361,\n",
       "       23494, 31963, 17056, 15937, 20703, 38343, 20960, 46039, 15613,\n",
       "        6425,  6771, 45594, 26062, 18443, 37662, 34127, 19701, 12163,\n",
       "       47406, 31279, 16270, 13829, 45526, 42022, 31794, 19643,  1298,\n",
       "       17181, 34583, 14609, 33694,  7460,  7687, 48202,  3683,  4465,\n",
       "       39603,  6657, 13557, 13396,  7989, 12782, 25722, 46674, 38028,\n",
       "       24772, 28419, 48830, 24881, 39722, 47364, 47253, 38728, 17921,\n",
       "        3730, 27110, 34999,  2481,   219, 29003, 39679, 30211,  3161,\n",
       "        6419,  5845,  8688, 49319,  2146, 24492,  1342, 18672, 44411,\n",
       "       26335, 44385, 47744, 19492, 43318, 46245,   998, 10218, 29348,\n",
       "        7513,  4461, 21472, 44372,  1426,  7370, 27993,  5420,  1823,\n",
       "       23771, 46540, 32616, 19083,  1906, 44015, 39677, 24666, 36762,\n",
       "       36612,  1408, 33044, 28471, 23690,  1339, 13166,  9910, 23229,\n",
       "       16468, 13478, 44411,  2515,  4192, 22099, 14276, 40139, 43438,\n",
       "       12529,  3889, 12168,  2329, 25059, 42592, 23820, 45012,  8296,\n",
       "        2412,  3988, 13027,  3765,  1565, 24480, 31064, 31367, 14740,\n",
       "       12771,  7935, 45507, 45974, 14577, 31255, 32009, 14113, 22543,\n",
       "       49293, 27502, 26706, 48540, 20397,  6124,  7664, 12359, 41073,\n",
       "       31810, 47390, 33473, 41235, 46522, 39083, 34580,  2204, 10458,\n",
       "       13736, 42192,  9438,  5398, 43096, 41379, 30713, 26335, 33367,\n",
       "       19710, 18232, 21052, 22546, 35391, 12844, 39210, 44056, 10975,\n",
       "       35628,  7840,  6824, 24136, 13366, 46132, 20846, 43699, 32736,\n",
       "       21836,    44, 17380, 26877, 12720, 14837, 15258,  6002,  5890,\n",
       "       46918, 19614, 14144, 33045, 39365, 18230, 11417, 38789, 11245,\n",
       "       28701, 48302, 48564,  4143, 31769, 23934, 11374, 48918, 48978,\n",
       "        2529, 28212, 42840, 22040, 32174, 21049, 22548, 18333, 22406,\n",
       "       21139, 13562, 36790, 19505,  4045,  9558,  9128,  4576,   161,\n",
       "       46685,  2580,  6868, 28152, 47623, 39592, 40587, 26014,  4493,\n",
       "       35468,  8576, 15203, 35848,  4591,  7635, 15256, 31713, 30069,\n",
       "       16059, 13352, 33075, 30526, 44181, 28863,  1929, 24971,   129,\n",
       "        5404, 39852, 12180, 24882, 32633, 43321, 26539, 28872, 46355,\n",
       "       25889, 16925, 42098, 14365, 22302,  9858, 18731, 49797, 19739,\n",
       "       20714,  2691, 22195, 31652, 37926, 34718,    66, 40422, 13060,\n",
       "       36052,  4641,  7146, 40775, 26252, 13857, 21163, 32030, 13120,\n",
       "       45401,  8854, 25088, 12617, 22843,  1350, 19505, 23100, 19392,\n",
       "       36669, 13152, 37228, 19128, 10580, 37676, 43860, 47147, 12473,\n",
       "        8092, 15885, 23228, 12545, 34837, 25841, 13542,   827, 20918,\n",
       "       37015, 15591,  2394, 30840, 27714,  9397, 31523, 24184, 34872,\n",
       "       34701, 18368, 38226, 11479, 34359, 34235,   390, 46927, 12954,\n",
       "         251, 20171, 45891, 23908,  5471,  5442, 10346,  8865,  4685,\n",
       "       48165, 15470, 39739, 16689, 10639, 33848, 30097,  7535, 33293,\n",
       "       12214, 47684,  2976, 39540, 12822, 21721,  9677, 34793, 23313,\n",
       "       14385, 22814, 24124, 18655, 30438, 27967, 47907, 44397, 27365,\n",
       "         233,  8483, 25458,  9110, 45809, 16227, 20946, 21353,  5977,\n",
       "       49036,  3603, 45699, 34460, 41847, 37326, 36243])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rand_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[train_rand_idxs]\n",
    "Y_train = Y_train[train_rand_idxs]\n",
    "X_val = X_val[val_rand_idxs]\n",
    "Y_val = Y_val[val_rand_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "Y_test = np_utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 1/70 [..............................] - ETA: 0s - loss: 2.3844 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.2982 - accuracy: 0.1071 - val_loss: 2.2767 - val_accuracy: 0.1067\n",
      "Epoch 2/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 2.2545 - accuracy: 0.1043 - val_loss: 2.2115 - val_accuracy: 0.1367\n",
      "Epoch 3/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 2.1912 - accuracy: 0.1414 - val_loss: 2.1377 - val_accuracy: 0.1900\n",
      "Epoch 4/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 2.1317 - accuracy: 0.1800 - val_loss: 2.0753 - val_accuracy: 0.1967\n",
      "Epoch 5/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 2.0815 - accuracy: 0.1886 - val_loss: 2.0288 - val_accuracy: 0.2333\n",
      "Epoch 6/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 2.0336 - accuracy: 0.2400 - val_loss: 1.9894 - val_accuracy: 0.2833\n",
      "Epoch 7/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.9857 - accuracy: 0.2871 - val_loss: 1.9479 - val_accuracy: 0.2833\n",
      "Epoch 8/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.9408 - accuracy: 0.2900 - val_loss: 1.9061 - val_accuracy: 0.2933\n",
      "Epoch 9/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.8942 - accuracy: 0.2929 - val_loss: 1.8717 - val_accuracy: 0.2800\n",
      "Epoch 10/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.8520 - accuracy: 0.2886 - val_loss: 1.8416 - val_accuracy: 0.2667\n",
      "Epoch 11/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.8158 - accuracy: 0.2814 - val_loss: 1.8106 - val_accuracy: 0.2700\n",
      "Epoch 12/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.7829 - accuracy: 0.2829 - val_loss: 1.7881 - val_accuracy: 0.2600\n",
      "Epoch 13/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.7543 - accuracy: 0.2986 - val_loss: 1.7660 - val_accuracy: 0.2700\n",
      "Epoch 14/300\n",
      "70/70 [==============================] - 0s 928us/step - loss: 1.7299 - accuracy: 0.3157 - val_loss: 1.7478 - val_accuracy: 0.2900\n",
      "Epoch 15/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.7067 - accuracy: 0.3114 - val_loss: 1.7308 - val_accuracy: 0.3000\n",
      "Epoch 16/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.6861 - accuracy: 0.3271 - val_loss: 1.7166 - val_accuracy: 0.3000\n",
      "Epoch 17/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.6668 - accuracy: 0.3457 - val_loss: 1.6988 - val_accuracy: 0.3133\n",
      "Epoch 18/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.6494 - accuracy: 0.3471 - val_loss: 1.6842 - val_accuracy: 0.3000\n",
      "Epoch 19/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.6332 - accuracy: 0.3486 - val_loss: 1.6751 - val_accuracy: 0.3067\n",
      "Epoch 20/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.6157 - accuracy: 0.3500 - val_loss: 1.6590 - val_accuracy: 0.2933\n",
      "Epoch 21/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.6024 - accuracy: 0.3657 - val_loss: 1.6484 - val_accuracy: 0.3267\n",
      "Epoch 22/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.5869 - accuracy: 0.3614 - val_loss: 1.6395 - val_accuracy: 0.3300\n",
      "Epoch 23/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.5733 - accuracy: 0.4014 - val_loss: 1.6325 - val_accuracy: 0.3400\n",
      "Epoch 24/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.5627 - accuracy: 0.3700 - val_loss: 1.6136 - val_accuracy: 0.3433\n",
      "Epoch 25/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.5487 - accuracy: 0.4014 - val_loss: 1.5992 - val_accuracy: 0.3700\n",
      "Epoch 26/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.5360 - accuracy: 0.3986 - val_loss: 1.5931 - val_accuracy: 0.3400\n",
      "Epoch 27/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5250 - accuracy: 0.3943 - val_loss: 1.5822 - val_accuracy: 0.3633\n",
      "Epoch 28/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.5129 - accuracy: 0.4043 - val_loss: 1.5797 - val_accuracy: 0.3500\n",
      "Epoch 29/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.5013 - accuracy: 0.4071 - val_loss: 1.5720 - val_accuracy: 0.3567\n",
      "Epoch 30/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.4919 - accuracy: 0.4100 - val_loss: 1.5556 - val_accuracy: 0.3533\n",
      "Epoch 31/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.4794 - accuracy: 0.4286 - val_loss: 1.5496 - val_accuracy: 0.3633\n",
      "Epoch 32/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.4679 - accuracy: 0.4214 - val_loss: 1.5409 - val_accuracy: 0.3467\n",
      "Epoch 33/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.4592 - accuracy: 0.4100 - val_loss: 1.5390 - val_accuracy: 0.3400\n",
      "Epoch 34/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.4499 - accuracy: 0.4300 - val_loss: 1.5313 - val_accuracy: 0.3767\n",
      "Epoch 35/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.4407 - accuracy: 0.4486 - val_loss: 1.5198 - val_accuracy: 0.3600\n",
      "Epoch 36/300\n",
      "70/70 [==============================] - 0s 972us/step - loss: 1.4305 - accuracy: 0.4557 - val_loss: 1.5060 - val_accuracy: 0.3800\n",
      "Epoch 37/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.4229 - accuracy: 0.4386 - val_loss: 1.5062 - val_accuracy: 0.3733\n",
      "Epoch 38/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.4119 - accuracy: 0.4529 - val_loss: 1.5001 - val_accuracy: 0.4000\n",
      "Epoch 39/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.4045 - accuracy: 0.4471 - val_loss: 1.5009 - val_accuracy: 0.4067\n",
      "Epoch 40/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.3977 - accuracy: 0.4686 - val_loss: 1.4889 - val_accuracy: 0.4133\n",
      "Epoch 41/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.3870 - accuracy: 0.4657 - val_loss: 1.4815 - val_accuracy: 0.4100\n",
      "Epoch 42/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.3806 - accuracy: 0.4657 - val_loss: 1.4802 - val_accuracy: 0.4133\n",
      "Epoch 43/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.3714 - accuracy: 0.4814 - val_loss: 1.4663 - val_accuracy: 0.4200\n",
      "Epoch 44/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.3650 - accuracy: 0.4914 - val_loss: 1.4606 - val_accuracy: 0.4200\n",
      "Epoch 45/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.3565 - accuracy: 0.4843 - val_loss: 1.4634 - val_accuracy: 0.4367\n",
      "Epoch 46/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.3519 - accuracy: 0.4886 - val_loss: 1.4537 - val_accuracy: 0.4367\n",
      "Epoch 47/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.3441 - accuracy: 0.4929 - val_loss: 1.4545 - val_accuracy: 0.4200\n",
      "Epoch 48/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.3368 - accuracy: 0.4843 - val_loss: 1.4422 - val_accuracy: 0.4533\n",
      "Epoch 49/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.3264 - accuracy: 0.5029 - val_loss: 1.4633 - val_accuracy: 0.4633\n",
      "Epoch 50/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.3235 - accuracy: 0.5214 - val_loss: 1.4396 - val_accuracy: 0.4400\n",
      "Epoch 51/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.3164 - accuracy: 0.5000 - val_loss: 1.4369 - val_accuracy: 0.4500\n",
      "Epoch 52/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 1.3113 - accuracy: 0.5114 - val_loss: 1.4220 - val_accuracy: 0.4567\n",
      "Epoch 53/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.3047 - accuracy: 0.5071 - val_loss: 1.4260 - val_accuracy: 0.4733\n",
      "Epoch 54/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.2969 - accuracy: 0.5257 - val_loss: 1.4157 - val_accuracy: 0.4633\n",
      "Epoch 55/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.2926 - accuracy: 0.5314 - val_loss: 1.4117 - val_accuracy: 0.4667\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 886us/step - loss: 1.2856 - accuracy: 0.5086 - val_loss: 1.4153 - val_accuracy: 0.4733\n",
      "Epoch 57/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.2812 - accuracy: 0.5214 - val_loss: 1.4072 - val_accuracy: 0.4700\n",
      "Epoch 58/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.2736 - accuracy: 0.5486 - val_loss: 1.3983 - val_accuracy: 0.4733\n",
      "Epoch 59/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.2704 - accuracy: 0.5414 - val_loss: 1.3986 - val_accuracy: 0.4867\n",
      "Epoch 60/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.2627 - accuracy: 0.5143 - val_loss: 1.3987 - val_accuracy: 0.4833\n",
      "Epoch 61/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.2575 - accuracy: 0.5414 - val_loss: 1.3876 - val_accuracy: 0.4933\n",
      "Epoch 62/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.2506 - accuracy: 0.5271 - val_loss: 1.3837 - val_accuracy: 0.4867\n",
      "Epoch 63/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.2463 - accuracy: 0.5414 - val_loss: 1.3953 - val_accuracy: 0.4867\n",
      "Epoch 64/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.2423 - accuracy: 0.5586 - val_loss: 1.4021 - val_accuracy: 0.5000\n",
      "Epoch 65/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.2334 - accuracy: 0.5386 - val_loss: 1.3728 - val_accuracy: 0.4933\n",
      "Epoch 66/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.2277 - accuracy: 0.5629 - val_loss: 1.3741 - val_accuracy: 0.4933\n",
      "Epoch 67/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.2291 - accuracy: 0.5600 - val_loss: 1.3817 - val_accuracy: 0.4900\n",
      "Epoch 68/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.2207 - accuracy: 0.5729 - val_loss: 1.3764 - val_accuracy: 0.4933\n",
      "Epoch 69/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.2194 - accuracy: 0.5457 - val_loss: 1.3769 - val_accuracy: 0.5133\n",
      "Epoch 70/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.2128 - accuracy: 0.5657 - val_loss: 1.3653 - val_accuracy: 0.4933\n",
      "Epoch 71/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.2077 - accuracy: 0.5757 - val_loss: 1.3682 - val_accuracy: 0.4900\n",
      "Epoch 72/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1995 - accuracy: 0.5557 - val_loss: 1.3727 - val_accuracy: 0.5133\n",
      "Epoch 73/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.1980 - accuracy: 0.5714 - val_loss: 1.3642 - val_accuracy: 0.4867\n",
      "Epoch 74/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.1917 - accuracy: 0.5614 - val_loss: 1.3765 - val_accuracy: 0.4933\n",
      "Epoch 75/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1906 - accuracy: 0.5843 - val_loss: 1.3598 - val_accuracy: 0.4933\n",
      "Epoch 76/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1837 - accuracy: 0.5714 - val_loss: 1.3855 - val_accuracy: 0.4733\n",
      "Epoch 77/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.1810 - accuracy: 0.5857 - val_loss: 1.3617 - val_accuracy: 0.4867\n",
      "Epoch 78/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1763 - accuracy: 0.5914 - val_loss: 1.3639 - val_accuracy: 0.5100\n",
      "Epoch 79/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1743 - accuracy: 0.5800 - val_loss: 1.3602 - val_accuracy: 0.5000\n",
      "Epoch 80/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1688 - accuracy: 0.5714 - val_loss: 1.3569 - val_accuracy: 0.5167\n",
      "Epoch 81/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.1621 - accuracy: 0.5943 - val_loss: 1.3626 - val_accuracy: 0.4933\n",
      "Epoch 82/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1607 - accuracy: 0.5929 - val_loss: 1.3581 - val_accuracy: 0.5233\n",
      "Epoch 83/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.1573 - accuracy: 0.5857 - val_loss: 1.3563 - val_accuracy: 0.5033\n",
      "Epoch 84/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1537 - accuracy: 0.5786 - val_loss: 1.3497 - val_accuracy: 0.5200\n",
      "Epoch 85/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1486 - accuracy: 0.5943 - val_loss: 1.3487 - val_accuracy: 0.4833\n",
      "Epoch 86/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.1455 - accuracy: 0.5957 - val_loss: 1.3418 - val_accuracy: 0.5167\n",
      "Epoch 87/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.1415 - accuracy: 0.5857 - val_loss: 1.3409 - val_accuracy: 0.5167\n",
      "Epoch 88/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.1368 - accuracy: 0.5829 - val_loss: 1.3568 - val_accuracy: 0.5200\n",
      "Epoch 89/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.1353 - accuracy: 0.5900 - val_loss: 1.3444 - val_accuracy: 0.5167\n",
      "Epoch 90/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.1285 - accuracy: 0.5971 - val_loss: 1.3489 - val_accuracy: 0.5567\n",
      "Epoch 91/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.1229 - accuracy: 0.6186 - val_loss: 1.3693 - val_accuracy: 0.4900\n",
      "Epoch 92/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1264 - accuracy: 0.5843 - val_loss: 1.3433 - val_accuracy: 0.5000\n",
      "Epoch 93/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.1178 - accuracy: 0.6157 - val_loss: 1.3456 - val_accuracy: 0.5033\n",
      "Epoch 94/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1156 - accuracy: 0.6071 - val_loss: 1.3323 - val_accuracy: 0.5300\n",
      "Epoch 95/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1122 - accuracy: 0.6143 - val_loss: 1.3334 - val_accuracy: 0.5167\n",
      "Epoch 96/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.1066 - accuracy: 0.6229 - val_loss: 1.3352 - val_accuracy: 0.5100\n",
      "Epoch 97/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.1036 - accuracy: 0.6157 - val_loss: 1.3455 - val_accuracy: 0.5100\n",
      "Epoch 98/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.1011 - accuracy: 0.6114 - val_loss: 1.3388 - val_accuracy: 0.5133\n",
      "Epoch 99/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0995 - accuracy: 0.6029 - val_loss: 1.3303 - val_accuracy: 0.5100\n",
      "Epoch 100/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0972 - accuracy: 0.6129 - val_loss: 1.3346 - val_accuracy: 0.5033\n",
      "Epoch 101/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.0907 - accuracy: 0.6171 - val_loss: 1.3307 - val_accuracy: 0.5633\n",
      "Epoch 102/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0882 - accuracy: 0.6157 - val_loss: 1.3352 - val_accuracy: 0.5400\n",
      "Epoch 103/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.0822 - accuracy: 0.6271 - val_loss: 1.3397 - val_accuracy: 0.5700\n",
      "Epoch 104/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0843 - accuracy: 0.6286 - val_loss: 1.3322 - val_accuracy: 0.4933\n",
      "Epoch 105/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0771 - accuracy: 0.6300 - val_loss: 1.3413 - val_accuracy: 0.5033\n",
      "Epoch 106/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.0762 - accuracy: 0.6143 - val_loss: 1.3224 - val_accuracy: 0.5000\n",
      "Epoch 107/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0735 - accuracy: 0.6229 - val_loss: 1.3351 - val_accuracy: 0.4967\n",
      "Epoch 108/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0704 - accuracy: 0.6129 - val_loss: 1.3233 - val_accuracy: 0.5000\n",
      "Epoch 109/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.0688 - accuracy: 0.6129 - val_loss: 1.3281 - val_accuracy: 0.5200\n",
      "Epoch 110/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0660 - accuracy: 0.6314 - val_loss: 1.3392 - val_accuracy: 0.5100\n",
      "Epoch 111/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0585 - accuracy: 0.6286 - val_loss: 1.3310 - val_accuracy: 0.5367\n",
      "Epoch 112/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0587 - accuracy: 0.6257 - val_loss: 1.3475 - val_accuracy: 0.5200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.0543 - accuracy: 0.6257 - val_loss: 1.3217 - val_accuracy: 0.5300\n",
      "Epoch 114/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.0528 - accuracy: 0.6271 - val_loss: 1.3249 - val_accuracy: 0.5267\n",
      "Epoch 115/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0524 - accuracy: 0.6343 - val_loss: 1.3209 - val_accuracy: 0.5100\n",
      "Epoch 116/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.0493 - accuracy: 0.6243 - val_loss: 1.3326 - val_accuracy: 0.5267\n",
      "Epoch 117/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0439 - accuracy: 0.6471 - val_loss: 1.3187 - val_accuracy: 0.5300\n",
      "Epoch 118/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0430 - accuracy: 0.6300 - val_loss: 1.3155 - val_accuracy: 0.5167\n",
      "Epoch 119/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0389 - accuracy: 0.6471 - val_loss: 1.3317 - val_accuracy: 0.4900\n",
      "Epoch 120/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0373 - accuracy: 0.6443 - val_loss: 1.3326 - val_accuracy: 0.5133\n",
      "Epoch 121/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0339 - accuracy: 0.6314 - val_loss: 1.3320 - val_accuracy: 0.5233\n",
      "Epoch 122/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.0297 - accuracy: 0.6529 - val_loss: 1.3176 - val_accuracy: 0.5367\n",
      "Epoch 123/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.0299 - accuracy: 0.6429 - val_loss: 1.3318 - val_accuracy: 0.5100\n",
      "Epoch 124/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0291 - accuracy: 0.6457 - val_loss: 1.3321 - val_accuracy: 0.5267\n",
      "Epoch 125/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0215 - accuracy: 0.6543 - val_loss: 1.3335 - val_accuracy: 0.5000\n",
      "Epoch 126/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0227 - accuracy: 0.6429 - val_loss: 1.3173 - val_accuracy: 0.5300\n",
      "Epoch 127/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.0185 - accuracy: 0.6514 - val_loss: 1.3341 - val_accuracy: 0.5000\n",
      "Epoch 128/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0165 - accuracy: 0.6500 - val_loss: 1.3363 - val_accuracy: 0.4933\n",
      "Epoch 129/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0124 - accuracy: 0.6614 - val_loss: 1.3156 - val_accuracy: 0.5500\n",
      "Epoch 130/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0131 - accuracy: 0.6429 - val_loss: 1.3113 - val_accuracy: 0.5333\n",
      "Epoch 131/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0064 - accuracy: 0.6557 - val_loss: 1.3242 - val_accuracy: 0.5167\n",
      "Epoch 132/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0070 - accuracy: 0.6614 - val_loss: 1.3180 - val_accuracy: 0.5233\n",
      "Epoch 133/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0036 - accuracy: 0.6500 - val_loss: 1.3337 - val_accuracy: 0.5267\n",
      "Epoch 134/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0002 - accuracy: 0.6671 - val_loss: 1.3293 - val_accuracy: 0.5067\n",
      "Epoch 135/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9985 - accuracy: 0.6500 - val_loss: 1.3388 - val_accuracy: 0.5367\n",
      "Epoch 136/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9987 - accuracy: 0.6657 - val_loss: 1.3336 - val_accuracy: 0.5133\n",
      "Epoch 137/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9949 - accuracy: 0.6471 - val_loss: 1.3286 - val_accuracy: 0.5233\n",
      "Epoch 138/300\n",
      "70/70 [==============================] - 0s 872us/step - loss: 0.9905 - accuracy: 0.6657 - val_loss: 1.3180 - val_accuracy: 0.5200\n",
      "Epoch 139/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.9898 - accuracy: 0.6557 - val_loss: 1.3222 - val_accuracy: 0.5600\n",
      "Epoch 140/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.9901 - accuracy: 0.6571 - val_loss: 1.3208 - val_accuracy: 0.5367\n",
      "Epoch 141/300\n",
      "70/70 [==============================] - 0s 872us/step - loss: 0.9864 - accuracy: 0.6657 - val_loss: 1.3250 - val_accuracy: 0.5133\n",
      "Epoch 142/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.9844 - accuracy: 0.6686 - val_loss: 1.3175 - val_accuracy: 0.5333\n",
      "Epoch 143/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.9803 - accuracy: 0.6686 - val_loss: 1.3216 - val_accuracy: 0.5100\n",
      "Epoch 144/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.9781 - accuracy: 0.6571 - val_loss: 1.3231 - val_accuracy: 0.5300\n",
      "Epoch 145/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9771 - accuracy: 0.6729 - val_loss: 1.3309 - val_accuracy: 0.5267\n",
      "Epoch 146/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.9747 - accuracy: 0.6671 - val_loss: 1.3243 - val_accuracy: 0.5167\n",
      "Epoch 147/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9736 - accuracy: 0.6686 - val_loss: 1.3406 - val_accuracy: 0.5233\n",
      "Epoch 148/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.9682 - accuracy: 0.6629 - val_loss: 1.3404 - val_accuracy: 0.5433\n",
      "Epoch 149/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.9715 - accuracy: 0.6571 - val_loss: 1.3240 - val_accuracy: 0.5500\n",
      "Epoch 150/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.9645 - accuracy: 0.6771 - val_loss: 1.3130 - val_accuracy: 0.5400\n",
      "Epoch 151/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.9624 - accuracy: 0.6729 - val_loss: 1.3370 - val_accuracy: 0.5467\n",
      "Epoch 152/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.9629 - accuracy: 0.6757 - val_loss: 1.3314 - val_accuracy: 0.5500\n",
      "Epoch 153/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.9592 - accuracy: 0.6657 - val_loss: 1.3310 - val_accuracy: 0.5467\n",
      "Epoch 154/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.9566 - accuracy: 0.6714 - val_loss: 1.3282 - val_accuracy: 0.5700\n",
      "Epoch 155/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.9557 - accuracy: 0.6600 - val_loss: 1.3497 - val_accuracy: 0.5533\n",
      "Epoch 156/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.9568 - accuracy: 0.6743 - val_loss: 1.3217 - val_accuracy: 0.5500\n",
      "Epoch 157/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.9537 - accuracy: 0.6743 - val_loss: 1.3403 - val_accuracy: 0.5100\n",
      "Epoch 158/300\n",
      "70/70 [==============================] - 0s 829us/step - loss: 0.9515 - accuracy: 0.6743 - val_loss: 1.3304 - val_accuracy: 0.5400\n",
      "Epoch 159/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.9480 - accuracy: 0.6700 - val_loss: 1.3295 - val_accuracy: 0.5400\n",
      "Epoch 160/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.9470 - accuracy: 0.6786 - val_loss: 1.3251 - val_accuracy: 0.5300\n",
      "Epoch 161/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.9440 - accuracy: 0.6729 - val_loss: 1.3358 - val_accuracy: 0.5533\n",
      "Epoch 162/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9411 - accuracy: 0.6657 - val_loss: 1.3312 - val_accuracy: 0.5300\n",
      "Epoch 163/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9405 - accuracy: 0.6671 - val_loss: 1.3728 - val_accuracy: 0.5400\n",
      "Epoch 164/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9347 - accuracy: 0.6786 - val_loss: 1.3286 - val_accuracy: 0.5600\n",
      "Epoch 165/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9381 - accuracy: 0.6714 - val_loss: 1.3321 - val_accuracy: 0.5500\n",
      "Epoch 166/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9357 - accuracy: 0.6843 - val_loss: 1.3440 - val_accuracy: 0.5400\n",
      "Epoch 167/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.9336 - accuracy: 0.6743 - val_loss: 1.3402 - val_accuracy: 0.5433\n",
      "Epoch 168/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9323 - accuracy: 0.6900 - val_loss: 1.3449 - val_accuracy: 0.5500\n",
      "Epoch 169/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 900us/step - loss: 0.9288 - accuracy: 0.6857 - val_loss: 1.3393 - val_accuracy: 0.5433\n",
      "Epoch 170/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.9265 - accuracy: 0.6829 - val_loss: 1.3493 - val_accuracy: 0.5467\n",
      "Epoch 171/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.9240 - accuracy: 0.6686 - val_loss: 1.3281 - val_accuracy: 0.5667\n",
      "Epoch 172/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.9226 - accuracy: 0.6929 - val_loss: 1.3327 - val_accuracy: 0.5267\n",
      "Epoch 173/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9217 - accuracy: 0.6686 - val_loss: 1.3419 - val_accuracy: 0.5600\n",
      "Epoch 174/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9210 - accuracy: 0.6729 - val_loss: 1.3460 - val_accuracy: 0.5533\n",
      "Epoch 175/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9201 - accuracy: 0.6743 - val_loss: 1.3502 - val_accuracy: 0.5500\n",
      "Epoch 176/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.9156 - accuracy: 0.6914 - val_loss: 1.3477 - val_accuracy: 0.5467\n",
      "Epoch 177/300\n",
      "70/70 [==============================] - 0s 872us/step - loss: 0.9175 - accuracy: 0.6900 - val_loss: 1.3678 - val_accuracy: 0.5433\n",
      "Epoch 178/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9140 - accuracy: 0.6757 - val_loss: 1.3443 - val_accuracy: 0.5567\n",
      "Epoch 179/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.9145 - accuracy: 0.6700 - val_loss: 1.3515 - val_accuracy: 0.5500\n",
      "Epoch 180/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9115 - accuracy: 0.6929 - val_loss: 1.3571 - val_accuracy: 0.5500\n",
      "Epoch 181/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9081 - accuracy: 0.6900 - val_loss: 1.3599 - val_accuracy: 0.5367\n",
      "Epoch 182/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.9068 - accuracy: 0.6814 - val_loss: 1.3630 - val_accuracy: 0.5367\n",
      "Epoch 183/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.9102 - accuracy: 0.6829 - val_loss: 1.3514 - val_accuracy: 0.5600\n",
      "Epoch 184/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9017 - accuracy: 0.6986 - val_loss: 1.3483 - val_accuracy: 0.5467\n",
      "Epoch 185/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.9040 - accuracy: 0.6900 - val_loss: 1.3518 - val_accuracy: 0.5567\n",
      "Epoch 186/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8984 - accuracy: 0.6943 - val_loss: 1.3437 - val_accuracy: 0.5567\n",
      "Epoch 187/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.9011 - accuracy: 0.6986 - val_loss: 1.3511 - val_accuracy: 0.5467\n",
      "Epoch 188/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8981 - accuracy: 0.6929 - val_loss: 1.3660 - val_accuracy: 0.5500\n",
      "Epoch 189/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8979 - accuracy: 0.6900 - val_loss: 1.3500 - val_accuracy: 0.5667\n",
      "Epoch 190/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8939 - accuracy: 0.7014 - val_loss: 1.3700 - val_accuracy: 0.5400\n",
      "Epoch 191/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.8958 - accuracy: 0.6900 - val_loss: 1.3441 - val_accuracy: 0.5300\n",
      "Epoch 192/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8921 - accuracy: 0.6814 - val_loss: 1.3673 - val_accuracy: 0.5400\n",
      "Epoch 193/300\n",
      "70/70 [==============================] - 0s 972us/step - loss: 0.8913 - accuracy: 0.7100 - val_loss: 1.3574 - val_accuracy: 0.5500\n",
      "Epoch 194/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.8903 - accuracy: 0.6914 - val_loss: 1.3582 - val_accuracy: 0.5467\n",
      "Epoch 195/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8867 - accuracy: 0.6971 - val_loss: 1.3715 - val_accuracy: 0.5433\n",
      "Epoch 196/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8861 - accuracy: 0.7100 - val_loss: 1.3608 - val_accuracy: 0.5500\n",
      "Epoch 197/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8848 - accuracy: 0.6943 - val_loss: 1.3772 - val_accuracy: 0.5400\n",
      "Epoch 198/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8830 - accuracy: 0.6971 - val_loss: 1.3549 - val_accuracy: 0.5433\n",
      "Epoch 199/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.8814 - accuracy: 0.6886 - val_loss: 1.3674 - val_accuracy: 0.5500\n",
      "Epoch 200/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8807 - accuracy: 0.6886 - val_loss: 1.3663 - val_accuracy: 0.5367\n",
      "Epoch 201/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8784 - accuracy: 0.6929 - val_loss: 1.3935 - val_accuracy: 0.5367\n",
      "Epoch 202/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8739 - accuracy: 0.6929 - val_loss: 1.3613 - val_accuracy: 0.5267\n",
      "Epoch 203/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8743 - accuracy: 0.7000 - val_loss: 1.3766 - val_accuracy: 0.5500\n",
      "Epoch 204/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.8715 - accuracy: 0.7014 - val_loss: 1.3909 - val_accuracy: 0.5367\n",
      "Epoch 205/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8725 - accuracy: 0.6971 - val_loss: 1.3705 - val_accuracy: 0.5500\n",
      "Epoch 206/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8673 - accuracy: 0.6943 - val_loss: 1.3694 - val_accuracy: 0.5433\n",
      "Epoch 207/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8669 - accuracy: 0.7057 - val_loss: 1.3738 - val_accuracy: 0.5367\n",
      "Epoch 208/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8688 - accuracy: 0.7043 - val_loss: 1.3682 - val_accuracy: 0.5367\n",
      "Epoch 209/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8656 - accuracy: 0.7029 - val_loss: 1.3858 - val_accuracy: 0.5167\n",
      "Epoch 210/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8629 - accuracy: 0.7157 - val_loss: 1.3856 - val_accuracy: 0.5367\n",
      "Epoch 211/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.8625 - accuracy: 0.7129 - val_loss: 1.3871 - val_accuracy: 0.5367\n",
      "Epoch 212/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8612 - accuracy: 0.7043 - val_loss: 1.3765 - val_accuracy: 0.5400\n",
      "Epoch 213/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.8608 - accuracy: 0.7100 - val_loss: 1.3833 - val_accuracy: 0.5433\n",
      "Epoch 214/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8587 - accuracy: 0.7086 - val_loss: 1.3769 - val_accuracy: 0.5367\n",
      "Epoch 215/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8572 - accuracy: 0.7157 - val_loss: 1.3807 - val_accuracy: 0.5400\n",
      "Epoch 216/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.8548 - accuracy: 0.7143 - val_loss: 1.3713 - val_accuracy: 0.5567\n",
      "Epoch 217/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8541 - accuracy: 0.7114 - val_loss: 1.3725 - val_accuracy: 0.5533\n",
      "Epoch 218/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8526 - accuracy: 0.7100 - val_loss: 1.3969 - val_accuracy: 0.5367\n",
      "Epoch 219/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8513 - accuracy: 0.7114 - val_loss: 1.3991 - val_accuracy: 0.5367\n",
      "Epoch 220/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.8515 - accuracy: 0.7086 - val_loss: 1.3828 - val_accuracy: 0.5367\n",
      "Epoch 221/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.8477 - accuracy: 0.7114 - val_loss: 1.3730 - val_accuracy: 0.5300\n",
      "Epoch 222/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8484 - accuracy: 0.7114 - val_loss: 1.4248 - val_accuracy: 0.5300\n",
      "Epoch 223/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8460 - accuracy: 0.7043 - val_loss: 1.3996 - val_accuracy: 0.5467\n",
      "Epoch 224/300\n",
      "70/70 [==============================] - 0s 843us/step - loss: 0.8456 - accuracy: 0.7286 - val_loss: 1.3822 - val_accuracy: 0.5400\n",
      "Epoch 225/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8394 - accuracy: 0.7129 - val_loss: 1.3962 - val_accuracy: 0.5500\n",
      "Epoch 226/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8435 - accuracy: 0.7157 - val_loss: 1.3740 - val_accuracy: 0.5367\n",
      "Epoch 227/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8412 - accuracy: 0.7200 - val_loss: 1.3984 - val_accuracy: 0.5333\n",
      "Epoch 228/300\n",
      "70/70 [==============================] - 0s 972us/step - loss: 0.8412 - accuracy: 0.7086 - val_loss: 1.3982 - val_accuracy: 0.5433\n",
      "Epoch 229/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8371 - accuracy: 0.7186 - val_loss: 1.3933 - val_accuracy: 0.5500\n",
      "Epoch 230/300\n",
      "70/70 [==============================] - 0s 829us/step - loss: 0.8359 - accuracy: 0.7300 - val_loss: 1.4021 - val_accuracy: 0.5267\n",
      "Epoch 231/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8343 - accuracy: 0.7200 - val_loss: 1.3872 - val_accuracy: 0.5333\n",
      "Epoch 232/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8312 - accuracy: 0.7229 - val_loss: 1.3960 - val_accuracy: 0.5400\n",
      "Epoch 233/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8325 - accuracy: 0.7300 - val_loss: 1.4080 - val_accuracy: 0.5333\n",
      "Epoch 234/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.8350 - accuracy: 0.7129 - val_loss: 1.4022 - val_accuracy: 0.5400\n",
      "Epoch 235/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8289 - accuracy: 0.7243 - val_loss: 1.3892 - val_accuracy: 0.5367\n",
      "Epoch 236/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.8271 - accuracy: 0.7214 - val_loss: 1.3898 - val_accuracy: 0.5400\n",
      "Epoch 237/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8287 - accuracy: 0.7100 - val_loss: 1.3955 - val_accuracy: 0.5400\n",
      "Epoch 238/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8261 - accuracy: 0.7329 - val_loss: 1.4107 - val_accuracy: 0.5333\n",
      "Epoch 239/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.8261 - accuracy: 0.7214 - val_loss: 1.4114 - val_accuracy: 0.5433\n",
      "Epoch 240/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.8243 - accuracy: 0.7157 - val_loss: 1.4170 - val_accuracy: 0.5433\n",
      "Epoch 241/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8226 - accuracy: 0.7386 - val_loss: 1.4152 - val_accuracy: 0.5467\n",
      "Epoch 242/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8251 - accuracy: 0.7229 - val_loss: 1.3944 - val_accuracy: 0.5433\n",
      "Epoch 243/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8235 - accuracy: 0.7257 - val_loss: 1.4073 - val_accuracy: 0.5400\n",
      "Epoch 244/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.8216 - accuracy: 0.7229 - val_loss: 1.3914 - val_accuracy: 0.5367\n",
      "Epoch 245/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.8170 - accuracy: 0.7229 - val_loss: 1.4059 - val_accuracy: 0.5433\n",
      "Epoch 246/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8152 - accuracy: 0.7343 - val_loss: 1.3994 - val_accuracy: 0.5467\n",
      "Epoch 247/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8138 - accuracy: 0.7314 - val_loss: 1.4110 - val_accuracy: 0.5167\n",
      "Epoch 248/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.8163 - accuracy: 0.7143 - val_loss: 1.4139 - val_accuracy: 0.5267\n",
      "Epoch 249/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8151 - accuracy: 0.7271 - val_loss: 1.4087 - val_accuracy: 0.5367\n",
      "Epoch 250/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8099 - accuracy: 0.7271 - val_loss: 1.3970 - val_accuracy: 0.5433\n",
      "Epoch 251/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.8110 - accuracy: 0.7243 - val_loss: 1.4172 - val_accuracy: 0.5367\n",
      "Epoch 252/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8105 - accuracy: 0.7243 - val_loss: 1.4039 - val_accuracy: 0.5333\n",
      "Epoch 253/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8070 - accuracy: 0.7314 - val_loss: 1.4013 - val_accuracy: 0.5367\n",
      "Epoch 254/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.8075 - accuracy: 0.7257 - val_loss: 1.4106 - val_accuracy: 0.5400\n",
      "Epoch 255/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.8058 - accuracy: 0.7371 - val_loss: 1.4133 - val_accuracy: 0.5333\n",
      "Epoch 256/300\n",
      "70/70 [==============================] - 0s 843us/step - loss: 0.8068 - accuracy: 0.7429 - val_loss: 1.4054 - val_accuracy: 0.5433\n",
      "Epoch 257/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8014 - accuracy: 0.7271 - val_loss: 1.4545 - val_accuracy: 0.5333\n",
      "Epoch 258/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8036 - accuracy: 0.7357 - val_loss: 1.4077 - val_accuracy: 0.5467\n",
      "Epoch 259/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8000 - accuracy: 0.7429 - val_loss: 1.4239 - val_accuracy: 0.5433\n",
      "Epoch 260/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.8000 - accuracy: 0.7400 - val_loss: 1.4317 - val_accuracy: 0.5600\n",
      "Epoch 261/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.8010 - accuracy: 0.7171 - val_loss: 1.4111 - val_accuracy: 0.5500\n",
      "Epoch 262/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.7968 - accuracy: 0.7443 - val_loss: 1.4143 - val_accuracy: 0.5300\n",
      "Epoch 263/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7955 - accuracy: 0.7357 - val_loss: 1.4353 - val_accuracy: 0.5433\n",
      "Epoch 264/300\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.7953 - accuracy: 0.7329 - val_loss: 1.4178 - val_accuracy: 0.5333\n",
      "Epoch 265/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.7969 - accuracy: 0.7329 - val_loss: 1.4271 - val_accuracy: 0.5433\n",
      "Epoch 266/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7947 - accuracy: 0.7429 - val_loss: 1.4349 - val_accuracy: 0.5300\n",
      "Epoch 267/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.7931 - accuracy: 0.7414 - val_loss: 1.4444 - val_accuracy: 0.5267\n",
      "Epoch 268/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.7920 - accuracy: 0.7400 - val_loss: 1.4537 - val_accuracy: 0.5300\n",
      "Epoch 269/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7892 - accuracy: 0.7371 - val_loss: 1.4250 - val_accuracy: 0.5367\n",
      "Epoch 270/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7891 - accuracy: 0.7471 - val_loss: 1.4480 - val_accuracy: 0.5267\n",
      "Epoch 271/300\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.7910 - accuracy: 0.7329 - val_loss: 1.4319 - val_accuracy: 0.5333\n",
      "Epoch 272/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.7895 - accuracy: 0.7414 - val_loss: 1.4355 - val_accuracy: 0.5267\n",
      "Epoch 273/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7871 - accuracy: 0.7257 - val_loss: 1.4319 - val_accuracy: 0.5333\n",
      "Epoch 274/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7862 - accuracy: 0.7457 - val_loss: 1.4260 - val_accuracy: 0.5400\n",
      "Epoch 275/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7859 - accuracy: 0.7429 - val_loss: 1.4204 - val_accuracy: 0.5500\n",
      "Epoch 276/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7853 - accuracy: 0.7386 - val_loss: 1.4294 - val_accuracy: 0.5367\n",
      "Epoch 277/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7824 - accuracy: 0.7457 - val_loss: 1.4434 - val_accuracy: 0.5300\n",
      "Epoch 278/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7805 - accuracy: 0.7371 - val_loss: 1.4426 - val_accuracy: 0.5267\n",
      "Epoch 279/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7800 - accuracy: 0.7443 - val_loss: 1.4435 - val_accuracy: 0.5233\n",
      "Epoch 280/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7798 - accuracy: 0.7443 - val_loss: 1.4431 - val_accuracy: 0.5333\n",
      "Epoch 281/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7778 - accuracy: 0.7443 - val_loss: 1.4396 - val_accuracy: 0.5367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7763 - accuracy: 0.7486 - val_loss: 1.4323 - val_accuracy: 0.5300\n",
      "Epoch 283/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.7767 - accuracy: 0.7557 - val_loss: 1.4362 - val_accuracy: 0.5333\n",
      "Epoch 284/300\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.7764 - accuracy: 0.7357 - val_loss: 1.4693 - val_accuracy: 0.5500\n",
      "Epoch 285/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7755 - accuracy: 0.7443 - val_loss: 1.4427 - val_accuracy: 0.5300\n",
      "Epoch 286/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7724 - accuracy: 0.7543 - val_loss: 1.4493 - val_accuracy: 0.5333\n",
      "Epoch 287/300\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7724 - accuracy: 0.7500 - val_loss: 1.4589 - val_accuracy: 0.5400\n",
      "Epoch 288/300\n",
      "70/70 [==============================] - 0s 857us/step - loss: 0.7709 - accuracy: 0.7457 - val_loss: 1.4426 - val_accuracy: 0.5367\n",
      "Epoch 289/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.7709 - accuracy: 0.7529 - val_loss: 1.4524 - val_accuracy: 0.5267\n",
      "Epoch 290/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.7672 - accuracy: 0.7471 - val_loss: 1.4781 - val_accuracy: 0.5467\n",
      "Epoch 291/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.7696 - accuracy: 0.7457 - val_loss: 1.4495 - val_accuracy: 0.5433\n",
      "Epoch 292/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.7679 - accuracy: 0.7429 - val_loss: 1.4523 - val_accuracy: 0.5433\n",
      "Epoch 293/300\n",
      "70/70 [==============================] - 0s 871us/step - loss: 0.7672 - accuracy: 0.7543 - val_loss: 1.4528 - val_accuracy: 0.5300\n",
      "Epoch 294/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.7668 - accuracy: 0.7457 - val_loss: 1.4513 - val_accuracy: 0.5367\n",
      "Epoch 295/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.7678 - accuracy: 0.7543 - val_loss: 1.4514 - val_accuracy: 0.5433\n",
      "Epoch 296/300\n",
      "70/70 [==============================] - 0s 886us/step - loss: 0.7643 - accuracy: 0.7571 - val_loss: 1.4630 - val_accuracy: 0.5467\n",
      "Epoch 297/300\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.7603 - accuracy: 0.7557 - val_loss: 1.4466 - val_accuracy: 0.5433\n",
      "Epoch 298/300\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.7597 - accuracy: 0.7429 - val_loss: 1.4515 - val_accuracy: 0.5367\n",
      "Epoch 299/300\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.7628 - accuracy: 0.7471 - val_loss: 1.4715 - val_accuracy: 0.5400\n",
      "Epoch 300/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.7573 - accuracy: 0.7486 - val_loss: 1.4909 - val_accuracy: 0.5167\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(X_train, Y_train, epochs=300, batch_size=10, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 866us/step - loss: 1.6227 - accuracy: 0.5155\n",
      "\n",
      "loss : 1.6226810216903687\n",
      "accuray : 0.515500009059906\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "print('')\n",
    "print('loss : ' + str(loss_and_metrics[0]))\n",
    "print('accuray : ' + str(loss_and_metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "es=EarlyStopping(patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.2727 - accuracy: 0.1543 - val_loss: 2.2600 - val_accuracy: 0.1633\n",
      "Epoch 2/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 2.2143 - accuracy: 0.2200 - val_loss: 2.2082 - val_accuracy: 0.2400\n",
      "Epoch 3/500\n",
      "70/70 [==============================] - 0s 972us/step - loss: 2.1492 - accuracy: 0.2757 - val_loss: 2.1558 - val_accuracy: 0.2600\n",
      "Epoch 4/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 2.0868 - accuracy: 0.2929 - val_loss: 2.0995 - val_accuracy: 0.2700\n",
      "Epoch 5/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 2.0286 - accuracy: 0.3014 - val_loss: 2.0500 - val_accuracy: 0.2733\n",
      "Epoch 6/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.9766 - accuracy: 0.3114 - val_loss: 2.0062 - val_accuracy: 0.2933\n",
      "Epoch 7/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.9291 - accuracy: 0.3214 - val_loss: 1.9695 - val_accuracy: 0.2967\n",
      "Epoch 8/500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.8890 - accuracy: 0.3314 - val_loss: 1.9331 - val_accuracy: 0.3067\n",
      "Epoch 9/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.8518 - accuracy: 0.3443 - val_loss: 1.9021 - val_accuracy: 0.3067\n",
      "Epoch 10/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.8192 - accuracy: 0.3471 - val_loss: 1.8734 - val_accuracy: 0.3167\n",
      "Epoch 11/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.7899 - accuracy: 0.3500 - val_loss: 1.8487 - val_accuracy: 0.3133\n",
      "Epoch 12/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.7629 - accuracy: 0.3529 - val_loss: 1.8293 - val_accuracy: 0.3200\n",
      "Epoch 13/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.7374 - accuracy: 0.3529 - val_loss: 1.8089 - val_accuracy: 0.3133\n",
      "Epoch 14/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.7163 - accuracy: 0.3514 - val_loss: 1.7841 - val_accuracy: 0.3267\n",
      "Epoch 15/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.6951 - accuracy: 0.3614 - val_loss: 1.7654 - val_accuracy: 0.3300\n",
      "Epoch 16/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.6754 - accuracy: 0.3643 - val_loss: 1.7495 - val_accuracy: 0.3333\n",
      "Epoch 17/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.6559 - accuracy: 0.3714 - val_loss: 1.7365 - val_accuracy: 0.3333\n",
      "Epoch 18/500\n",
      "70/70 [==============================] - 0s 857us/step - loss: 1.6397 - accuracy: 0.3786 - val_loss: 1.7206 - val_accuracy: 0.3367\n",
      "Epoch 19/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.6238 - accuracy: 0.3729 - val_loss: 1.7107 - val_accuracy: 0.3233\n",
      "Epoch 20/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.6088 - accuracy: 0.3857 - val_loss: 1.6965 - val_accuracy: 0.3333\n",
      "Epoch 21/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5940 - accuracy: 0.3814 - val_loss: 1.6821 - val_accuracy: 0.3400\n",
      "Epoch 22/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5796 - accuracy: 0.3857 - val_loss: 1.6713 - val_accuracy: 0.3333\n",
      "Epoch 23/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5668 - accuracy: 0.3843 - val_loss: 1.6585 - val_accuracy: 0.3400\n",
      "Epoch 24/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.5553 - accuracy: 0.3943 - val_loss: 1.6505 - val_accuracy: 0.3367\n",
      "Epoch 25/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.5426 - accuracy: 0.3986 - val_loss: 1.6446 - val_accuracy: 0.3333\n",
      "Epoch 26/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.5308 - accuracy: 0.3929 - val_loss: 1.6319 - val_accuracy: 0.3400\n",
      "Epoch 27/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5207 - accuracy: 0.4014 - val_loss: 1.6267 - val_accuracy: 0.3367\n",
      "Epoch 28/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.5101 - accuracy: 0.4000 - val_loss: 1.6166 - val_accuracy: 0.3433\n",
      "Epoch 29/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.5006 - accuracy: 0.4086 - val_loss: 1.6131 - val_accuracy: 0.3333\n",
      "Epoch 30/500\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.4908 - accuracy: 0.4057 - val_loss: 1.6031 - val_accuracy: 0.3433\n",
      "Epoch 31/500\n",
      "70/70 [==============================] - 0s 872us/step - loss: 1.4815 - accuracy: 0.4029 - val_loss: 1.5994 - val_accuracy: 0.3433\n",
      "Epoch 32/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.4731 - accuracy: 0.4071 - val_loss: 1.5930 - val_accuracy: 0.3333\n",
      "Epoch 33/500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.4639 - accuracy: 0.4086 - val_loss: 1.5910 - val_accuracy: 0.3367\n",
      "Epoch 34/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.4549 - accuracy: 0.4114 - val_loss: 1.5848 - val_accuracy: 0.3367\n",
      "Epoch 35/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.4459 - accuracy: 0.4143 - val_loss: 1.5756 - val_accuracy: 0.3567\n",
      "Epoch 36/500\n",
      "70/70 [==============================] - 0s 872us/step - loss: 1.4380 - accuracy: 0.4157 - val_loss: 1.5690 - val_accuracy: 0.3500\n",
      "Epoch 37/500\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.4308 - accuracy: 0.4143 - val_loss: 1.5650 - val_accuracy: 0.3600\n",
      "Epoch 38/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.4228 - accuracy: 0.4314 - val_loss: 1.5593 - val_accuracy: 0.3567\n",
      "Epoch 39/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.4149 - accuracy: 0.4171 - val_loss: 1.5539 - val_accuracy: 0.3600\n",
      "Epoch 40/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.4076 - accuracy: 0.4257 - val_loss: 1.5514 - val_accuracy: 0.3700\n",
      "Epoch 41/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.4009 - accuracy: 0.4371 - val_loss: 1.5439 - val_accuracy: 0.3667\n",
      "Epoch 42/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.3930 - accuracy: 0.4300 - val_loss: 1.5457 - val_accuracy: 0.3700\n",
      "Epoch 43/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.3874 - accuracy: 0.4357 - val_loss: 1.5376 - val_accuracy: 0.3600\n",
      "Epoch 44/500\n",
      "70/70 [==============================] - 0s 843us/step - loss: 1.3798 - accuracy: 0.4343 - val_loss: 1.5384 - val_accuracy: 0.3633\n",
      "Epoch 45/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.3741 - accuracy: 0.4371 - val_loss: 1.5271 - val_accuracy: 0.3733\n",
      "Epoch 46/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.3665 - accuracy: 0.4457 - val_loss: 1.5243 - val_accuracy: 0.3733\n",
      "Epoch 47/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.3615 - accuracy: 0.4457 - val_loss: 1.5182 - val_accuracy: 0.3867\n",
      "Epoch 48/500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.3542 - accuracy: 0.4471 - val_loss: 1.5119 - val_accuracy: 0.3900\n",
      "Epoch 49/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.3498 - accuracy: 0.4557 - val_loss: 1.5136 - val_accuracy: 0.3933\n",
      "Epoch 50/500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.3415 - accuracy: 0.4629 - val_loss: 1.5125 - val_accuracy: 0.3900\n",
      "Epoch 51/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.3363 - accuracy: 0.4486 - val_loss: 1.5072 - val_accuracy: 0.4000\n",
      "Epoch 52/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.3305 - accuracy: 0.4586 - val_loss: 1.5008 - val_accuracy: 0.4067\n",
      "Epoch 53/500\n",
      "70/70 [==============================] - 0s 857us/step - loss: 1.3252 - accuracy: 0.4629 - val_loss: 1.4968 - val_accuracy: 0.3967\n",
      "Epoch 54/500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 1.3175 - accuracy: 0.4629 - val_loss: 1.5041 - val_accuracy: 0.4000\n",
      "Epoch 55/500\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.3146 - accuracy: 0.4957 - val_loss: 1.4961 - val_accuracy: 0.4000\n",
      "Epoch 56/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.3082 - accuracy: 0.5000 - val_loss: 1.4896 - val_accuracy: 0.4500\n",
      "Epoch 57/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.3006 - accuracy: 0.5171 - val_loss: 1.4885 - val_accuracy: 0.4467\n",
      "Epoch 58/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 900us/step - loss: 1.2959 - accuracy: 0.5200 - val_loss: 1.4901 - val_accuracy: 0.4467\n",
      "Epoch 59/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.2913 - accuracy: 0.5171 - val_loss: 1.4804 - val_accuracy: 0.4400\n",
      "Epoch 60/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.2848 - accuracy: 0.5243 - val_loss: 1.4817 - val_accuracy: 0.4500\n",
      "Epoch 61/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.2790 - accuracy: 0.5186 - val_loss: 1.4766 - val_accuracy: 0.4400\n",
      "Epoch 62/500\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.2744 - accuracy: 0.5371 - val_loss: 1.4739 - val_accuracy: 0.4467\n",
      "Epoch 63/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.2690 - accuracy: 0.5286 - val_loss: 1.4713 - val_accuracy: 0.4433\n",
      "Epoch 64/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.2646 - accuracy: 0.5343 - val_loss: 1.4683 - val_accuracy: 0.4367\n",
      "Epoch 65/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.2584 - accuracy: 0.5314 - val_loss: 1.4641 - val_accuracy: 0.4500\n",
      "Epoch 66/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.2533 - accuracy: 0.5386 - val_loss: 1.4636 - val_accuracy: 0.4433\n",
      "Epoch 67/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.2480 - accuracy: 0.5343 - val_loss: 1.4579 - val_accuracy: 0.4533\n",
      "Epoch 68/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.2445 - accuracy: 0.5386 - val_loss: 1.4599 - val_accuracy: 0.4500\n",
      "Epoch 69/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.2378 - accuracy: 0.5443 - val_loss: 1.4536 - val_accuracy: 0.4567\n",
      "Epoch 70/500\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.2345 - accuracy: 0.5443 - val_loss: 1.4532 - val_accuracy: 0.4500\n",
      "Epoch 71/500\n",
      "70/70 [==============================] - 0s 857us/step - loss: 1.2281 - accuracy: 0.5471 - val_loss: 1.4504 - val_accuracy: 0.4567\n",
      "Epoch 72/500\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.2238 - accuracy: 0.5443 - val_loss: 1.4454 - val_accuracy: 0.4733\n",
      "Epoch 73/500\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.2192 - accuracy: 0.5471 - val_loss: 1.4531 - val_accuracy: 0.4567\n",
      "Epoch 74/500\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.2150 - accuracy: 0.5457 - val_loss: 1.4459 - val_accuracy: 0.4700\n",
      "Epoch 75/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.2107 - accuracy: 0.5500 - val_loss: 1.4404 - val_accuracy: 0.4700\n",
      "Epoch 76/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.2060 - accuracy: 0.5500 - val_loss: 1.4431 - val_accuracy: 0.4700\n",
      "Epoch 77/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.2005 - accuracy: 0.5557 - val_loss: 1.4460 - val_accuracy: 0.4667\n",
      "Epoch 78/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1971 - accuracy: 0.5486 - val_loss: 1.4333 - val_accuracy: 0.4767\n",
      "Epoch 79/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1920 - accuracy: 0.5629 - val_loss: 1.4342 - val_accuracy: 0.4733\n",
      "Epoch 80/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.1880 - accuracy: 0.5571 - val_loss: 1.4320 - val_accuracy: 0.4833\n",
      "Epoch 81/500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.1842 - accuracy: 0.5614 - val_loss: 1.4293 - val_accuracy: 0.4767\n",
      "Epoch 82/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.1791 - accuracy: 0.5586 - val_loss: 1.4325 - val_accuracy: 0.4867\n",
      "Epoch 83/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.1741 - accuracy: 0.5614 - val_loss: 1.4255 - val_accuracy: 0.4767\n",
      "Epoch 84/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.1710 - accuracy: 0.5657 - val_loss: 1.4290 - val_accuracy: 0.4833\n",
      "Epoch 85/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.1674 - accuracy: 0.5657 - val_loss: 1.4229 - val_accuracy: 0.4800\n",
      "Epoch 86/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.1627 - accuracy: 0.5629 - val_loss: 1.4217 - val_accuracy: 0.4833\n",
      "Epoch 87/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.1592 - accuracy: 0.5571 - val_loss: 1.4239 - val_accuracy: 0.4867\n",
      "Epoch 88/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.1555 - accuracy: 0.5643 - val_loss: 1.4157 - val_accuracy: 0.4733\n",
      "Epoch 89/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1521 - accuracy: 0.5729 - val_loss: 1.4190 - val_accuracy: 0.4733\n",
      "Epoch 90/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1493 - accuracy: 0.5643 - val_loss: 1.4174 - val_accuracy: 0.4867\n",
      "Epoch 91/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.1450 - accuracy: 0.5757 - val_loss: 1.4130 - val_accuracy: 0.4700\n",
      "Epoch 92/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1424 - accuracy: 0.5600 - val_loss: 1.4170 - val_accuracy: 0.4900\n",
      "Epoch 93/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1384 - accuracy: 0.5614 - val_loss: 1.4151 - val_accuracy: 0.4800\n",
      "Epoch 94/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1352 - accuracy: 0.5700 - val_loss: 1.4065 - val_accuracy: 0.4767\n",
      "Epoch 95/500\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.1307 - accuracy: 0.5757 - val_loss: 1.4039 - val_accuracy: 0.4733\n",
      "Epoch 96/500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.1293 - accuracy: 0.5671 - val_loss: 1.4035 - val_accuracy: 0.4767\n",
      "Epoch 97/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1267 - accuracy: 0.5714 - val_loss: 1.4048 - val_accuracy: 0.4967\n",
      "Epoch 98/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1206 - accuracy: 0.5743 - val_loss: 1.4025 - val_accuracy: 0.4767\n",
      "Epoch 99/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1208 - accuracy: 0.5686 - val_loss: 1.4057 - val_accuracy: 0.5000\n",
      "Epoch 100/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.1161 - accuracy: 0.5829 - val_loss: 1.4035 - val_accuracy: 0.4933\n",
      "Epoch 101/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1139 - accuracy: 0.5714 - val_loss: 1.4038 - val_accuracy: 0.5000\n",
      "Epoch 102/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1099 - accuracy: 0.5800 - val_loss: 1.3954 - val_accuracy: 0.4933\n",
      "Epoch 103/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.1059 - accuracy: 0.5900 - val_loss: 1.4080 - val_accuracy: 0.4933\n",
      "Epoch 104/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.1039 - accuracy: 0.5800 - val_loss: 1.3955 - val_accuracy: 0.5000\n",
      "Epoch 105/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0996 - accuracy: 0.5829 - val_loss: 1.3984 - val_accuracy: 0.5133\n",
      "Epoch 106/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0991 - accuracy: 0.5771 - val_loss: 1.3973 - val_accuracy: 0.5200\n",
      "Epoch 107/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0944 - accuracy: 0.5857 - val_loss: 1.3964 - val_accuracy: 0.5100\n",
      "Epoch 108/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0902 - accuracy: 0.5886 - val_loss: 1.3933 - val_accuracy: 0.5067\n",
      "Epoch 109/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0879 - accuracy: 0.5914 - val_loss: 1.3918 - val_accuracy: 0.5100\n",
      "Epoch 110/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0856 - accuracy: 0.5929 - val_loss: 1.3880 - val_accuracy: 0.5067\n",
      "Epoch 111/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0830 - accuracy: 0.5886 - val_loss: 1.3856 - val_accuracy: 0.5033\n",
      "Epoch 112/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0809 - accuracy: 0.5843 - val_loss: 1.3858 - val_accuracy: 0.5133\n",
      "Epoch 113/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0774 - accuracy: 0.5914 - val_loss: 1.3887 - val_accuracy: 0.5200\n",
      "Epoch 114/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0748 - accuracy: 0.5857 - val_loss: 1.3878 - val_accuracy: 0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0719 - accuracy: 0.5829 - val_loss: 1.3891 - val_accuracy: 0.5300\n",
      "Epoch 116/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0703 - accuracy: 0.6029 - val_loss: 1.3838 - val_accuracy: 0.5133\n",
      "Epoch 117/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0659 - accuracy: 0.5871 - val_loss: 1.3923 - val_accuracy: 0.5233\n",
      "Epoch 118/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0619 - accuracy: 0.6029 - val_loss: 1.3811 - val_accuracy: 0.5100\n",
      "Epoch 119/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0626 - accuracy: 0.5943 - val_loss: 1.3836 - val_accuracy: 0.5167\n",
      "Epoch 120/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0596 - accuracy: 0.5914 - val_loss: 1.3829 - val_accuracy: 0.5267\n",
      "Epoch 121/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0570 - accuracy: 0.6057 - val_loss: 1.3803 - val_accuracy: 0.5200\n",
      "Epoch 122/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0576 - accuracy: 0.6000 - val_loss: 1.3846 - val_accuracy: 0.5167\n",
      "Epoch 123/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0524 - accuracy: 0.5971 - val_loss: 1.3793 - val_accuracy: 0.5167\n",
      "Epoch 124/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0494 - accuracy: 0.6014 - val_loss: 1.3863 - val_accuracy: 0.5167\n",
      "Epoch 125/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0482 - accuracy: 0.6057 - val_loss: 1.3849 - val_accuracy: 0.5133\n",
      "Epoch 126/500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 1.0472 - accuracy: 0.5943 - val_loss: 1.3783 - val_accuracy: 0.5267\n",
      "Epoch 127/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0424 - accuracy: 0.6014 - val_loss: 1.3840 - val_accuracy: 0.5133\n",
      "Epoch 128/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0419 - accuracy: 0.6100 - val_loss: 1.3865 - val_accuracy: 0.5167\n",
      "Epoch 129/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.0390 - accuracy: 0.6100 - val_loss: 1.3856 - val_accuracy: 0.5100\n",
      "Epoch 130/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.0347 - accuracy: 0.5957 - val_loss: 1.3979 - val_accuracy: 0.5200\n",
      "Epoch 131/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0365 - accuracy: 0.6043 - val_loss: 1.3875 - val_accuracy: 0.5200\n",
      "Epoch 132/500\n",
      "70/70 [==============================] - 0s 886us/step - loss: 1.0338 - accuracy: 0.6114 - val_loss: 1.3845 - val_accuracy: 0.5300\n",
      "Epoch 133/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0322 - accuracy: 0.6129 - val_loss: 1.3826 - val_accuracy: 0.5200\n",
      "Epoch 134/500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.0299 - accuracy: 0.6000 - val_loss: 1.3870 - val_accuracy: 0.5233\n",
      "Epoch 135/500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 1.0265 - accuracy: 0.6086 - val_loss: 1.3837 - val_accuracy: 0.5233\n",
      "Epoch 136/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0235 - accuracy: 0.6100 - val_loss: 1.3913 - val_accuracy: 0.5100\n",
      "Epoch 137/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0244 - accuracy: 0.6157 - val_loss: 1.3878 - val_accuracy: 0.5200\n",
      "Epoch 138/500\n",
      "70/70 [==============================] - 0s 872us/step - loss: 1.0220 - accuracy: 0.6043 - val_loss: 1.3933 - val_accuracy: 0.5333\n",
      "Epoch 139/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0198 - accuracy: 0.6171 - val_loss: 1.3965 - val_accuracy: 0.5267\n",
      "Epoch 140/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0180 - accuracy: 0.6171 - val_loss: 1.4041 - val_accuracy: 0.5200\n",
      "Epoch 141/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0151 - accuracy: 0.6057 - val_loss: 1.3924 - val_accuracy: 0.5267\n",
      "Epoch 142/500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 1.0145 - accuracy: 0.6229 - val_loss: 1.3940 - val_accuracy: 0.5267\n",
      "Epoch 143/500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 1.0109 - accuracy: 0.6200 - val_loss: 1.3948 - val_accuracy: 0.5333\n",
      "Epoch 144/500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.0097 - accuracy: 0.6200 - val_loss: 1.3993 - val_accuracy: 0.5167\n",
      "Epoch 145/500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 1.0080 - accuracy: 0.6286 - val_loss: 1.3932 - val_accuracy: 0.5233\n",
      "Epoch 146/500\n",
      "70/70 [==============================] - 0s 871us/step - loss: 1.0035 - accuracy: 0.6300 - val_loss: 1.3916 - val_accuracy: 0.5233\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(X_train, Y_train, epochs=500, batch_size=10, validation_data=(X_val, Y_val), \n",
    "               callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 1.1070 - accuracy: 0.7239 - val_loss: 0.6516 - val_accuracy: 0.8362\n",
      "Epoch 2/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.5184 - accuracy: 0.8673 - val_loss: 0.4795 - val_accuracy: 0.8721\n",
      "Epoch 3/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.4161 - accuracy: 0.8853 - val_loss: 0.4168 - val_accuracy: 0.8855\n",
      "Epoch 4/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3696 - accuracy: 0.8973 - val_loss: 0.3843 - val_accuracy: 0.8932\n",
      "Epoch 5/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3407 - accuracy: 0.9039 - val_loss: 0.3600 - val_accuracy: 0.8987\n",
      "  1/313 [..............................] - ETA: 0s - loss: 0.2274 - accuracy: 0.9688WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "313/313 [==============================] - 0s 923us/step - loss: 0.3354 - accuracy: 0.9055\n",
      "\n",
      "loss_and_metrics : [0.33539122343063354, 0.9054999947547913]\n",
      "WARNING:tensorflow:From <ipython-input-19-cc256cab72d0>:47: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "True : 9, Predict : 9\n",
      "True : 9, Predict : 9\n",
      "True : 2, Predict : 2\n",
      "True : 1, Predict : 1\n",
      "True : 9, Predict : 9\n"
     ]
    }
   ],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 데이터셋 전처리\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 원핫인코딩 (one-hot encoding) 처리\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[:42000] # 훈련셋의 30%를 검증셋으로 사용\n",
    "x_train = x_train[42000:]\n",
    "y_val = y_train[:42000] # 훈련셋의 30%를 검증셋으로 사용\n",
    "y_train = y_train[42000:]\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('')\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
    "\n",
    "# 6. 모델 사용하기\n",
    "xhat_idx = np.random.choice(x_test.shape[0], 5)\n",
    "xhat = x_test[xhat_idx]\n",
    "yhat = model.predict_classes(xhat)\n",
    "\n",
    "for i in range(5):\n",
    "    print('True : ' + str(argmax(y_test[xhat_idx[i]])) + ', Predict : ' + str(yhat[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mymnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "xhat_idx = np.random.choice(x_test.shape[0], 5)\n",
    "xhat = x_test[xhat_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(\"mymnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat=model.predict_classes(xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True : 3, Predict : 3\n",
      "True : 5, Predict : 5\n",
      "True : 7, Predict : 7\n",
      "True : 0, Predict : 0\n",
      "True : 6, Predict : 6\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('True : ' + str(argmax(y_test[xhat_idx[i]])) + ', Predict : ' + str(yhat[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 이용한 수열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM, Embedding, SimpleRNN\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([SimpleRNN(units=1, activation='tanh', return_sequences=False,\n",
    "                     return_state=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. ]\n",
      "  [0.1]\n",
      "  [0.2]\n",
      "  [0.3]]\n",
      "\n",
      " [[0.1]\n",
      "  [0.2]\n",
      "  [0.3]\n",
      "  [0.4]]\n",
      "\n",
      " [[0.2]\n",
      "  [0.3]\n",
      "  [0.4]\n",
      "  [0.5]]\n",
      "\n",
      " [[0.3]\n",
      "  [0.4]\n",
      "  [0.5]\n",
      "  [0.6]]\n",
      "\n",
      " [[0.4]\n",
      "  [0.5]\n",
      "  [0.6]\n",
      "  [0.7]]\n",
      "\n",
      " [[0.5]\n",
      "  [0.6]\n",
      "  [0.7]\n",
      "  [0.8]]]\n",
      "[0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    }
   ],
   "source": [
    "X = [] \n",
    "Y = [] \n",
    "for i in range(6): \n",
    "    lst = list(range(i,i+4)) \n",
    "    X.append(list(map(lambda c: [c/10], lst))) \n",
    "    Y.append((i+4)/10) \n",
    "X = np.array(X) \n",
    "Y = np.array(Y) \n",
    "print(X)  #6,4,1\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([SimpleRNN(units=10, return_sequences=False, input_shape=[4,1]), Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0053\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0044\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0039\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0035\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0017\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0012\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.7430e-04\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 8.0369e-04\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4983e-04\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0763e-04\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7914e-04\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7042e-04\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8672e-04\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2872e-04\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 9.1605e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7430e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9261e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4215e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.3558e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0198e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5527e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.7693e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2356e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5205e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 7.4279e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.0402e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.5724e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.1645e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.7687e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0198e-04\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0282e-04\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.9968e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.4700e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.8761e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.3146e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.7674e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 7.1540e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4269e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6241e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.8440e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1739e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.6345e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.1845e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7710e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3797e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.0410e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7964e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6575e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5962e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5684e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5473e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5361e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5525e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6033e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6736e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7371e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7772e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7962e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8072e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.8186e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8260e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8184e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.7904e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7474e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7006e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6569e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6151e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5703e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5211e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.4721e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4296e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.3960e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3688e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3441e-0 - 0s 2ms/step - loss: 1.3441e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3211e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3021e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.2893e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2817e-0 - 0s 4ms/step - loss: 1.2817e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2764e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2709e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2652e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2605e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.2572e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2541e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2495e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2428e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2349e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 1.2268e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2185e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2092e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.1987e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1874e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1762e-05\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1655e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.1550e-05\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X, Y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3965483 ],\n",
       "       [0.501097  ],\n",
       "       [0.6034714 ],\n",
       "       [0.70351446],\n",
       "       [0.800743  ],\n",
       "       [0.8944696 ]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2928729]]\n"
     ]
    }
   ],
   "source": [
    "sooneung=np.array([[[1.0],[1.1],[1.2],[1.3]]])\n",
    "print(model.predict(sooneung))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 기반 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"인공지능을 공부하면서 코딩을 하고 있다\\n\n",
    "파이썬 코딩을 배우고 익혔다\\n\n",
    "딥러닝을 배우고 코딩을 하고 있다\\n\n",
    "파이썬 기반에서 판다스를 배우고 코딩을 했다\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'코딩을': 1,\n",
       " '배우고': 2,\n",
       " '하고': 3,\n",
       " '있다': 4,\n",
       " '파이썬': 5,\n",
       " '인공지능을': 6,\n",
       " '공부하면서': 7,\n",
       " '익혔다': 8,\n",
       " '딥러닝을': 9,\n",
       " '기반에서': 10,\n",
       " '판다스를': 11,\n",
       " '했다': 12}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('인공지능을', 1),\n",
       "             ('공부하면서', 1),\n",
       "             ('코딩을', 4),\n",
       "             ('하고', 2),\n",
       "             ('있다', 2),\n",
       "             ('파이썬', 2),\n",
       "             ('배우고', 3),\n",
       "             ('익혔다', 1),\n",
       "             ('딥러닝을', 1),\n",
       "             ('기반에서', 1),\n",
       "             ('판다스를', 1),\n",
       "             ('했다', 1)])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'): \n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 7],\n",
       " [6, 7, 1],\n",
       " [6, 7, 1, 3],\n",
       " [6, 7, 1, 3, 4],\n",
       " [5, 1],\n",
       " [5, 1, 2],\n",
       " [5, 1, 2, 8],\n",
       " [9, 2],\n",
       " [9, 2, 1],\n",
       " [9, 2, 1, 3],\n",
       " [9, 2, 1, 3, 4],\n",
       " [5, 10],\n",
       " [5, 10, 11],\n",
       " [5, 10, 11, 2],\n",
       " [5, 10, 11, 2, 1],\n",
       " [5, 10, 11, 2, 1, 12]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=max(len(i) for i in sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=pad_sequences(sequences, maxlen=max_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sequences[:,:-1]\n",
    "y=sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  1,  3,  4,  1,  2,  8,  2,  1,  3,  4, 10, 11,  2,  1, 12])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 크기 : 13\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(t.word_index)+1\n",
    "t.index_word#실제 단어는 12개, index : 0~12\n",
    "print(\"단어 크기 : %d\" % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y=to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential(Embedding(vocab_size, 10, input_length=max_len-1 )) #13차원 -> 10차원으로...embedding\n",
    "model.add(SimpleRNN(32)) #출력 dim:32\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5574 - accuracy: 0.1875\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.5438 - accuracy: 0.1875\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5301 - accuracy: 0.2500\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5162 - accuracy: 0.3125\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.5020 - accuracy: 0.2500\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.4875 - accuracy: 0.2500\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4724 - accuracy: 0.2500\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.4567 - accuracy: 0.2500\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.4403 - accuracy: 0.2500\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.4230 - accuracy: 0.2500\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.4050 - accuracy: 0.3125\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.3860 - accuracy: 0.3750\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3661 - accuracy: 0.3750\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3454 - accuracy: 0.3750\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.3238 - accuracy: 0.3750\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3015 - accuracy: 0.3750\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2787 - accuracy: 0.3125\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.2556 - accuracy: 0.2500\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2325 - accuracy: 0.2500\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2097 - accuracy: 0.2500\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1875 - accuracy: 0.2500\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.1663 - accuracy: 0.2500\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1465 - accuracy: 0.2500\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1283 - accuracy: 0.2500\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.1119 - accuracy: 0.2500\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0972 - accuracy: 0.2500\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0840 - accuracy: 0.2500\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0719 - accuracy: 0.2500\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0606 - accuracy: 0.2500\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.0495 - accuracy: 0.2500\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0384 - accuracy: 0.2500\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.0269 - accuracy: 0.2500\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.0151 - accuracy: 0.2500\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0027 - accuracy: 0.2500\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.9900 - accuracy: 0.2500\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9769 - accuracy: 0.2500\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.9637 - accuracy: 0.2500\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9503 - accuracy: 0.2500\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.9369 - accuracy: 0.2500\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.9234 - accuracy: 0.2500\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.9100 - accuracy: 0.2500\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8965 - accuracy: 0.2500\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8831 - accuracy: 0.2500\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8696 - accuracy: 0.2500\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8560 - accuracy: 0.2500\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8423 - accuracy: 0.3750\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8284 - accuracy: 0.4375\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.8144 - accuracy: 0.4375\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8003 - accuracy: 0.4375\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7860 - accuracy: 0.4375\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7715 - accuracy: 0.4375\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7568 - accuracy: 0.3750\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7419 - accuracy: 0.3750\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7267 - accuracy: 0.4375\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7114 - accuracy: 0.5000\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.6959 - accuracy: 0.5000\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.6803 - accuracy: 0.5000\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6645 - accuracy: 0.5625\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6486 - accuracy: 0.5625\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6328 - accuracy: 0.5625\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.6169 - accuracy: 0.5625\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6011 - accuracy: 0.5625\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5853 - accuracy: 0.5000\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.5695 - accuracy: 0.5000\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5538 - accuracy: 0.5000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5382 - accuracy: 0.5000\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5227 - accuracy: 0.5000\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.5072 - accuracy: 0.5000\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4918 - accuracy: 0.5000\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4765 - accuracy: 0.5000\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4613 - accuracy: 0.5000\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.4462 - accuracy: 0.5000\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4312 - accuracy: 0.5000\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4164 - accuracy: 0.5625\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4016 - accuracy: 0.5625\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.3870 - accuracy: 0.5625\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.3725 - accuracy: 0.5625\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.3582 - accuracy: 0.6250\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.3439 - accuracy: 0.6250\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3298 - accuracy: 0.6250\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3158 - accuracy: 0.6250\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.3019 - accuracy: 0.6250\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2881 - accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.2744 - accuracy: 0.6875\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2608 - accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.2472 - accuracy: 0.6875\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.2337 - accuracy: 0.6875\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2202 - accuracy: 0.6875\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2068 - accuracy: 0.6875\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1934 - accuracy: 0.7500\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1800 - accuracy: 0.7500\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1666 - accuracy: 0.7500\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.1532 - accuracy: 0.7500\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1399 - accuracy: 0.7500\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.1265 - accuracy: 0.7500\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.1132 - accuracy: 0.7500\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.0998 - accuracy: 0.7500\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.0865 - accuracy: 0.7500\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.0733 - accuracy: 0.7500\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.0601 - accuracy: 0.7500\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 996us/step - loss: 1.0469 - accuracy: 0.7500\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0338 - accuracy: 0.7500\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0207 - accuracy: 0.7500\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0078 - accuracy: 0.7500\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9949 - accuracy: 0.7500\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9822 - accuracy: 0.7500\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9695 - accuracy: 0.7500\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9570 - accuracy: 0.7500\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9446 - accuracy: 0.7500\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9324 - accuracy: 0.8125\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9203 - accuracy: 0.8125\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.9084 - accuracy: 0.8125\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8966 - accuracy: 0.8125\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8850 - accuracy: 0.8125\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8735 - accuracy: 0.8125\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8622 - accuracy: 0.8125\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8511 - accuracy: 0.8125\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8401 - accuracy: 0.8125\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8293 - accuracy: 0.8125\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8186 - accuracy: 0.8125\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8081 - accuracy: 0.8125\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7977 - accuracy: 0.8125\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7875 - accuracy: 0.8125\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7774 - accuracy: 0.8125\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7675 - accuracy: 0.8125\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7577 - accuracy: 0.8125\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7480 - accuracy: 0.8125\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7385 - accuracy: 0.8125\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7291 - accuracy: 0.8125\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7198 - accuracy: 0.8125\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7107 - accuracy: 0.8750\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7017 - accuracy: 0.8750\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.6928 - accuracy: 0.8750\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6841 - accuracy: 0.8750\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6755 - accuracy: 0.8750\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6670 - accuracy: 0.8750\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6586 - accuracy: 0.8750\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6503 - accuracy: 0.8750\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6422 - accuracy: 0.8750\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6342 - accuracy: 0.8750\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.8750\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6185 - accuracy: 0.8750\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6109 - accuracy: 0.8750\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6034 - accuracy: 0.8750\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5960 - accuracy: 0.8750\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5887 - accuracy: 0.8750\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5815 - accuracy: 0.8750\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.5744 - accuracy: 0.8750\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5675 - accuracy: 0.8750\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.5606 - accuracy: 0.8750\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5539 - accuracy: 0.8750\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.5473 - accuracy: 0.8750\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5408 - accuracy: 0.8750\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5344 - accuracy: 0.8750\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5281 - accuracy: 0.8750\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.5219 - accuracy: 0.8750\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5158 - accuracy: 0.8750\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.8750\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.8750\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.4981 - accuracy: 0.8750\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4924 - accuracy: 0.8750\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.8750\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4813 - accuracy: 0.8750\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.8750\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4705 - accuracy: 0.8750\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4653 - accuracy: 0.8750\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4601 - accuracy: 0.8750\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.8750\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4500 - accuracy: 0.8750\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4451 - accuracy: 0.8750\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4402 - accuracy: 0.8750\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.4355 - accuracy: 0.8750\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.4308 - accuracy: 0.8750\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4262 - accuracy: 0.8750\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.4216 - accuracy: 0.8750\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4172 - accuracy: 0.8750\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8750\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4085 - accuracy: 0.8750\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4042 - accuracy: 0.8750\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4000 - accuracy: 0.8750\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3959 - accuracy: 0.8750\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8750\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3879 - accuracy: 0.8750\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3840 - accuracy: 0.8750\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3801 - accuracy: 0.8750\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3763 - accuracy: 0.8750\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3726 - accuracy: 0.8750\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3689 - accuracy: 0.8750\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8750\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3617 - accuracy: 0.8750\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3582 - accuracy: 0.8750\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.8750\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3514 - accuracy: 0.8750\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.3481 - accuracy: 0.8750\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3448 - accuracy: 0.8750\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3415 - accuracy: 0.8750\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3384 - accuracy: 0.8750\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3352 - accuracy: 0.8750\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3322 - accuracy: 0.8750\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3291 - accuracy: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28aa642de80>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(X,y,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n):\n",
    "    init_word=current_word\n",
    "    sentence=''\n",
    "    for _ in range(n):\n",
    "        encoded=t.texts_to_sequences([current_word])[0] # 단어('인공지능을')의 인덱스\n",
    "        encoded=pad_sequences([encoded], maxlen=5, padding='pre')\n",
    "        result=model.predict_classes(encoded)\n",
    "        for word, index in t.word_index.items():\n",
    "            if index==result:\n",
    "                break\n",
    "        current_word=current_word+\" \"+word\n",
    "        sentence=sentence+\" \"+word\n",
    "    sentence=init_word+sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능을 배우고 코딩을 하고 있다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, \"인공지능을\", 4)) # 출력 : 인공지능을 공부하면서 코딩을 하고 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 7]]\n"
     ]
    }
   ],
   "source": [
    "t.word_index\n",
    "t.index_word\n",
    "print(t.texts_to_sequences(['인공지능을 공부하면서']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
